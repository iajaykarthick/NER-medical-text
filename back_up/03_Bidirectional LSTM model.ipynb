{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dense"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Remove punctuation and convert to lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text).lower()\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def load_data(data_dir, max_length=None):\n",
    "\n",
    "    # Load all files in the data directory\n",
    "    all_files = os.listdir(data_dir)\n",
    "\n",
    "    # Filter only the files with the .bio extension\n",
    "    bio_files = [f for f in all_files if f.endswith('.bio')]\n",
    "\n",
    "    # Initialize lists to hold sentences and labels\n",
    "    sentences = []\n",
    "    labels = []\n",
    "\n",
    "    # Loop through each file and read the sentences and labels\n",
    "    for file in bio_files:\n",
    "        with open(os.path.join(data_dir, file), 'r', encoding='utf-8') as f:\n",
    "            current_sentences = []\n",
    "            current_labels = []\n",
    "            for line in f:\n",
    "                if line.strip() == '':\n",
    "                    # If we encounter a blank line, it means we've reached the end of a sentence\n",
    "                    if len(current_sentences) > 0:\n",
    "                        # Add the current sentence and labels to the list\n",
    "                        sentences.append(current_sentences)\n",
    "                        labels.append(current_labels)\n",
    "                        # Reset the current sentence and labels lists\n",
    "                        current_sentences = []\n",
    "                        current_labels = []\n",
    "                else:\n",
    "                    # Otherwise, split the line into its word and label components\n",
    "                    word, label = line.strip().split('\\t')\n",
    "                    current_sentences.append(clean_text(word))\n",
    "                    current_labels.append(label)\n",
    "\n",
    "    # Shuffle the sentences and labels\n",
    "    combined = list(zip(sentences, labels))\n",
    "    random.shuffle(combined)\n",
    "    sentences[:], labels[:] = zip(*combined)\n",
    "\n",
    "    # Split the data into training, validation, and test sets\n",
    "    num_sentences = len(sentences)\n",
    "    num_train = int(num_sentences * 0.8)\n",
    "    num_valid = int(num_sentences * 0.1)\n",
    "\n",
    "    train_sentences = sentences[:num_train]\n",
    "    train_labels = labels[:num_train]\n",
    "    valid_sentences = sentences[num_train:num_train+num_valid]\n",
    "    valid_labels = labels[num_train:num_train+num_valid]\n",
    "    test_sentences = sentences[num_train+num_valid:]\n",
    "    test_labels = labels[num_train+num_valid:]\n",
    "\n",
    "    # Convert the labels to one-hot encoding\n",
    "    unique_labels = set(element for sublist in labels for element in sublist)\n",
    "    label_to_index = {label: id+1 for id, label in enumerate(sorted(unique_labels))}\n",
    "    index_to_label = {id: label for label, id in label_to_index.items()}\n",
    "\n",
    "    # Add the new label and ID to the dictionaries\n",
    "    label_to_index['<PAD>'] = 0\n",
    "    index_to_label[0] = '<PAD>'\n",
    "\n",
    "    num_classes = len(index_to_label) - 1\n",
    "\n",
    "    train_labels = [[label_to_index[label] for label in labels] for labels in train_labels]\n",
    "    train_labels = pad_sequences(train_labels, maxlen=max_length, padding='post', value=num_classes)\n",
    "    train_labels = to_categorical(train_labels, num_classes=num_classes+1)\n",
    "\n",
    "    valid_labels = [[label_to_index[label] for label in labels] for labels in valid_labels]\n",
    "    valid_labels = pad_sequences(valid_labels, maxlen=max_length, padding='post', value=num_classes)\n",
    "    valid_labels = to_categorical(valid_labels, num_classes=num_classes+1)\n",
    "\n",
    "    test_labels = [[label_to_index[label] for label in labels] for labels in test_labels]\n",
    "    test_labels = pad_sequences(test_labels, maxlen=max_length, padding='post', value=num_classes)\n",
    "    test_labels = to_categorical(test_labels, num_classes=num_classes+1)\n",
    "\n",
    "    return (train_sentences, train_labels), (valid_sentences, valid_labels), (test_sentences, test_labels), label_to_index, index_to_label"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "(train_sentences, train_labels), (val_sentences, val_labels), (test_sentences, test_labels), label2id, id2label = load_data('./data/BIO_FILES', 200)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[[0., 0., 0., ..., 0., 0., 1.],\n        [0., 0., 0., ..., 0., 0., 1.],\n        [0., 0., 0., ..., 0., 0., 1.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 1.],\n        [0., 0., 0., ..., 0., 0., 1.],\n        [0., 0., 0., ..., 0., 0., 1.]],\n\n       [[0., 0., 0., ..., 0., 0., 1.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 1.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 1.],\n        [0., 0., 0., ..., 0., 0., 1.],\n        [0., 0., 0., ..., 0., 0., 1.]],\n\n       [[0., 0., 0., ..., 0., 0., 1.],\n        [0., 0., 0., ..., 0., 0., 1.],\n        [0., 0., 0., ..., 0., 0., 1.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 1.],\n        [0., 0., 0., ..., 0., 0., 1.],\n        [0., 0., 0., ..., 0., 0., 1.]],\n\n       ...,\n\n       [[0., 0., 0., ..., 0., 0., 1.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 1.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 1.],\n        [0., 0., 0., ..., 0., 0., 1.],\n        [0., 0., 0., ..., 0., 0., 1.]],\n\n       [[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 1.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 1.],\n        [0., 0., 0., ..., 0., 0., 1.],\n        [0., 0., 0., ..., 0., 0., 1.]],\n\n       [[0., 0., 0., ..., 0., 0., 1.],\n        [0., 0., 0., ..., 0., 0., 1.],\n        [0., 0., 0., ..., 0., 0., 1.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 1.],\n        [0., 0., 0., ..., 0., 0., 1.],\n        [0., 0., 0., ..., 0., 0., 1.]]], dtype=float32)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 100000\n",
    "EMBEDDING_DIM = 128\n",
    "MAX_LENGTH = 200\n",
    "NUM_CLASSES = 35\n",
    "LSTM_UNITS = 64\n",
    "NUM_EPOCHS = 10"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Convert the input sentences to sequences of word indices\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "val_sequences = tokenizer.texts_to_sequences(val_sentences)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "train_sequences_padded = pad_sequences(train_sequences, maxlen=MAX_LENGTH, padding='post', truncating='post')\n",
    "val_sequences_padded = pad_sequences(val_sequences, maxlen=MAX_LENGTH, padding='post', truncating='post')\n",
    "test_sequences_padded = pad_sequences(test_sequences, maxlen=MAX_LENGTH, padding='post', truncating='post')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Define the model architecture\n",
    "model = tf.keras.models.Sequential([\n",
    "    Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_DIM, input_length=MAX_LENGTH),\n",
    "    Bidirectional(LSTM(units=LSTM_UNITS, return_sequences=True)),\n",
    "    Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n",
    "\n",
    "#\n",
    "# model = tf.keras.models.Sequential([\n",
    "#     Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_DIM, input_length=MAX_LENGTH),\n",
    "#     Bidirectional(LSTM(units=LSTM_UNITS, return_sequences=True)),\n",
    "#     Dense(64, activation='relu'),\n",
    "#     Dense(NUM_CLASSES, activation='softmax')\n",
    "# ])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 200, 128)          12800000  \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 200, 128)         98816     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 200, 35)           4515      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,903,331\n",
      "Trainable params: 12,903,331\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-03 10:09:02.430344: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114/114 [==============================] - 14s 106ms/step - loss: 0.5058 - accuracy: 0.9568 - val_loss: 0.1699 - val_accuracy: 0.9641\n",
      "Epoch 2/10\n",
      "114/114 [==============================] - 12s 109ms/step - loss: 0.1544 - accuracy: 0.9658 - val_loss: 0.1624 - val_accuracy: 0.9641\n",
      "Epoch 3/10\n",
      "114/114 [==============================] - 13s 114ms/step - loss: 0.1379 - accuracy: 0.9659 - val_loss: 0.1572 - val_accuracy: 0.9641\n",
      "Epoch 4/10\n",
      "114/114 [==============================] - 13s 112ms/step - loss: 0.1220 - accuracy: 0.9679 - val_loss: 0.1605 - val_accuracy: 0.9649\n",
      "Epoch 5/10\n",
      "114/114 [==============================] - 13s 111ms/step - loss: 0.1100 - accuracy: 0.9712 - val_loss: 0.1584 - val_accuracy: 0.9651\n",
      "Epoch 6/10\n",
      "114/114 [==============================] - 13s 112ms/step - loss: 0.0962 - accuracy: 0.9741 - val_loss: 0.1545 - val_accuracy: 0.9656\n",
      "Epoch 7/10\n",
      "114/114 [==============================] - 13s 110ms/step - loss: 0.0819 - accuracy: 0.9772 - val_loss: 0.1545 - val_accuracy: 0.9666\n",
      "Epoch 8/10\n",
      "114/114 [==============================] - 13s 114ms/step - loss: 0.0691 - accuracy: 0.9804 - val_loss: 0.1554 - val_accuracy: 0.9670\n",
      "Epoch 9/10\n",
      "114/114 [==============================] - 13s 115ms/step - loss: 0.0584 - accuracy: 0.9833 - val_loss: 0.1593 - val_accuracy: 0.9673\n",
      "Epoch 10/10\n",
      "114/114 [==============================] - 13s 115ms/step - loss: 0.0501 - accuracy: 0.9856 - val_loss: 0.1626 - val_accuracy: 0.9675\n",
      "15/15 [==============================] - 0s 31ms/step - loss: 0.1555 - accuracy: 0.9690\n",
      "Test accuracy: 0.9690219759941101\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(train_sequences_padded, train_labels, epochs=NUM_EPOCHS, validation_data=(val_sequences_padded, val_labels))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(test_sequences_padded, test_labels)\n",
    "\n",
    "# Print the test accuracy\n",
    "print('Test accuracy:', test_acc)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[[0., 0., 0., ..., 0., 0., 1.],\n        [0., 0., 0., ..., 0., 0., 1.],\n        [0., 0., 0., ..., 0., 0., 1.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 1.],\n        [0., 0., 0., ..., 0., 0., 1.],\n        [0., 0., 0., ..., 0., 0., 1.]],\n\n       [[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 1.],\n        [0., 0., 0., ..., 0., 0., 1.],\n        [0., 0., 0., ..., 0., 0., 1.]],\n\n       [[0., 0., 0., ..., 0., 0., 1.],\n        [0., 0., 0., ..., 0., 0., 1.],\n        [0., 0., 0., ..., 0., 0., 1.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 1.],\n        [0., 0., 0., ..., 0., 0., 1.],\n        [0., 0., 0., ..., 0., 0., 1.]],\n\n       ...,\n\n       [[0., 0., 0., ..., 0., 0., 1.],\n        [0., 0., 0., ..., 0., 0., 1.],\n        [0., 0., 0., ..., 0., 0., 1.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 1.],\n        [0., 0., 0., ..., 0., 0., 1.],\n        [0., 0., 0., ..., 0., 0., 1.]],\n\n       [[0., 0., 0., ..., 0., 0., 1.],\n        [0., 0., 0., ..., 0., 0., 1.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 1.],\n        [0., 0., 0., ..., 0., 0., 1.],\n        [0., 0., 0., ..., 0., 0., 1.]],\n\n       [[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 1.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 1.],\n        [0., 0., 0., ..., 0., 0., 1.],\n        [0., 0., 0., ..., 0., 0., 1.]]], dtype=float32)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 12ms/step\n",
      "Predicted Named Entities:\n",
      "The: O\n",
      "patient: O\n",
      "is: O\n",
      "a: O\n",
      "55-year-old: B-Therapeutic_procedure\n",
      "male: O\n",
      "with: O\n",
      "a: I-History\n",
      "history: I-History\n",
      "of: I-History\n",
      "hypertension: I-History\n",
      "and: I-History\n",
      "diabetes: I-History\n",
      "He: B-Clinical_event\n",
      "presented: O\n",
      "to: O\n",
      "the: O\n",
      "emergency: O\n",
      "department: O\n",
      "with: O\n",
      "complaints: O\n",
      "of: B-Biological_structure\n",
      "chest: B-Sign_symptom\n",
      "pain: B-Sign_symptom\n",
      "shortness: I-Sign_symptom\n",
      "of: I-Sign_symptom\n",
      "breath: O\n",
      "and: B-Sign_symptom\n",
      "dizziness: O\n",
      "The: O\n",
      "patient's: B-Diagnostic_procedure\n",
      "blood: I-Diagnostic_procedure\n",
      "pressure: O\n",
      "was: I-Lab_value\n",
      "180/110: O\n",
      "mmHg: O\n",
      "and: B-Diagnostic_procedure\n",
      "his: I-Diagnostic_procedure\n",
      "heart: O\n",
      "rate: B-Lab_value\n",
      "was: I-Lab_value\n",
      "110: I-Lab_value\n",
      "beats: I-Lab_value\n",
      "per: O\n",
      "minute: B-Diagnostic_procedure\n",
      "A: O\n",
      "12-lead: B-Sign_symptom\n",
      "electrocardiogram: I-Lab_value\n",
      "showed: O\n",
      "ST-segment: O\n",
      "elevation: B-Biological_structure\n",
      "in: O\n",
      "the: O\n",
      "anterior: O\n",
      "leads: O\n",
      "The: O\n",
      "patient: O\n",
      "was: O\n",
      "diagnosed: O\n",
      "with: B-Disease_disorder\n",
      "an: I-Disease_disorder\n",
      "acute: O\n",
      "myocardial: O\n",
      "infarction: O\n",
      "and: O\n",
      "was: O\n",
      "immediately: B-Medication\n",
      "started: O\n",
      "on: B-Medication\n",
      "heparin: O\n",
      "and: O\n",
      "aspirin: O\n",
      "therapy: O\n",
      "He: B-Disease_disorder\n",
      "underwent: I-Diagnostic_procedure\n",
      "a: O\n",
      "cardiac: O\n",
      "catheterization: O\n",
      "and: O\n",
      "was: O\n",
      "found: O\n",
      "to: B-Sign_symptom\n",
      "have: O\n",
      "significant: O\n",
      "stenosis: B-Biological_structure\n",
      "in: I-Biological_structure\n",
      "the: I-Biological_structure\n",
      "left: I-Biological_structure\n",
      "anterior: O\n",
      "descending: O\n",
      "artery: O\n",
      "He: B-Biological_structure\n",
      "underwent: B-Therapeutic_procedure\n",
      "percutaneous: O\n",
      "coronary: B-Therapeutic_procedure\n",
      "intervention: O\n",
      "with: O\n",
      "stent: O\n",
      "placement: O\n",
      "and: O\n",
      "his: O\n",
      "symptoms: O\n",
      "improved: B-Clinical_event\n",
      "He: O\n",
      "was: O\n",
      "discharged: B-Medication\n",
      "home: B-Medication\n",
      "on: O\n",
      "aspirin: B-Medication\n",
      "clopidogrel: O\n",
      "atorvastatin: O\n",
      "and: O\n",
      "lisinopril: O\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "def tokenize_text(text):\n",
    "    # Tokenize the text into a list of words\n",
    "    tokens = []\n",
    "    for sentence in text.split('\\n'):\n",
    "        for word in sentence.split():\n",
    "            # Remove trailing punctuation marks from the word\n",
    "            while word and word[-1] in string.punctuation:\n",
    "                word = word[:-1]\n",
    "            tokens.append(word)\n",
    "    return tokens\n",
    "\n",
    "text = \"The patient is a 55-year-old male with a history of hypertension and diabetes. He presented to the emergency department with complaints of chest pain, shortness of breath, and dizziness. The patient's blood pressure was 180/110 mmHg and his heart rate was 110 beats per minute. A 12-lead electrocardiogram showed ST-segment elevation in the anterior leads. The patient was diagnosed with an acute myocardial infarction and was immediately started on heparin and aspirin therapy. He underwent a cardiac catheterization and was found to have significant stenosis in the left anterior descending artery. He underwent percutaneous coronary intervention with stent placement and his symptoms improved. He was discharged home on aspirin, clopidogrel, atorvastatin, and lisinopril.\"\n",
    "\n",
    "\n",
    "# tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "tokens = tokenize_text(text)\n",
    "\n",
    "sequence = tokenizer.texts_to_sequences([' '.join(clean_text(token) for token in tokens)])\n",
    "padded_sequence = pad_sequences(sequence, maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "# Make the prediction\n",
    "prediction = model.predict(np.array(padded_sequence))\n",
    "\n",
    "# Decode the prediction\n",
    "predicted_labels = np.argmax(prediction, axis=-1)\n",
    "predicted_labels = [id2label[i] for i in predicted_labels[0]]\n",
    "\n",
    "# Print the predicted named entities\n",
    "print(\"Predicted Named Entities:\")\n",
    "for i in range(len(tokens)):\n",
    "    print(f\"{tokens[i]}: {''.join(predicted_labels[i])}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "['O',\n 'O',\n 'O',\n 'O',\n 'B-Therapeutic_procedure',\n 'O',\n 'O',\n 'I-History',\n 'I-History',\n 'I-History',\n 'I-History',\n 'I-History',\n 'I-History',\n 'B-Clinical_event',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'B-Biological_structure',\n 'B-Sign_symptom',\n 'B-Sign_symptom',\n 'I-Sign_symptom',\n 'I-Sign_symptom',\n 'O',\n 'B-Sign_symptom',\n 'O',\n 'O',\n 'B-Diagnostic_procedure',\n 'I-Diagnostic_procedure',\n 'O',\n 'I-Lab_value',\n 'O',\n 'O',\n 'B-Diagnostic_procedure',\n 'I-Diagnostic_procedure',\n 'O',\n 'B-Lab_value',\n 'I-Lab_value',\n 'I-Lab_value',\n 'I-Lab_value',\n 'O',\n 'B-Diagnostic_procedure',\n 'O',\n 'B-Sign_symptom',\n 'I-Lab_value',\n 'O',\n 'O',\n 'B-Biological_structure',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'B-Disease_disorder',\n 'I-Disease_disorder',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'B-Medication',\n 'O',\n 'B-Medication',\n 'O',\n 'O',\n 'O',\n 'O',\n 'B-Disease_disorder',\n 'I-Diagnostic_procedure',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'B-Sign_symptom',\n 'O',\n 'O',\n 'B-Biological_structure',\n 'I-Biological_structure',\n 'I-Biological_structure',\n 'I-Biological_structure',\n 'O',\n 'O',\n 'O',\n 'B-Biological_structure',\n 'B-Therapeutic_procedure',\n 'O',\n 'B-Therapeutic_procedure',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'B-Clinical_event',\n 'O',\n 'O',\n 'B-Medication',\n 'B-Medication',\n 'O',\n 'B-Medication',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O',\n 'O']"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[   1,   11,  145,    5,  280,    6,    5,   34,    3,  218,    2,\n         467,   19,   65,    8,    1,  224,  189,    6, 1500,    3,   50,\n          74,  607,    3,  410,    2, 2032,    1,   79,   32,   63,    4,\n         143,    2,   28,   78,  150,    4, 3456, 1232,  215, 1080,    5,\n        4605,  594,   13, 2145,  758,    7,    1,  262,  582,    1,   11,\n           4,   88,    6,   20,  200,  538, 1330,    2,    4,  546,  199,\n          12, 1114,    2, 1355,   77,   19,  108,    5,  111,  910,    2,\n           4,  137,    8,  216,  164,  529,    7,    1,   23,  262,  907,\n          83,   19,  108,  864,  174, 1010,    6,  542, 1332,    2,   28,\n         120,  323,   19,    4,  156,  434,   12, 1355,    2, 4116,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0]], dtype=int32)"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(padded_sequence)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[   1,   11,  145,    5,  280,    6,    5,   34,    3,  218,    2,\n         467,   19,   65,    8,    1,  224,  189,    6, 1500,    3,   50,\n          74,  607,    3,  410,    2, 2032,    1,   79,   32,   63,    4,\n         143,    2,   28,   78,  150,    4, 3456, 1232,  215, 1080,    5,\n        4605,  594,   13, 2145,  758,    7,    1,  262,  582,    1,   11,\n           4,   88,    6,   20,  200,  538, 1330,    2,    4,  546,  199,\n          12, 1114,    2, 1355,   77,   19,  108,    5,  111,  910,    2,\n           4,  137,    8,  216,  164,  529,    7,    1,   23,  262,  907,\n          83,   19,  108,  864,  174, 1010,    6,  542, 1332,    2,   28,\n         120,  323,   19,    4,  156,  434,   12, 1355,    2, 4116,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0]], dtype=int32)"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(padded_sequence)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1131)>\n"
     ]
    },
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 11ms/step\n",
      "Predicted Named Entities:\n",
      "The: O\n",
      "patient: O\n",
      "is: O\n",
      "a: O\n",
      "55: O\n",
      "year: O\n",
      "old: O\n",
      "male: O\n",
      "with: O\n",
      "a: O\n",
      "history: O\n",
      "of: O\n",
      "hypertension: O\n",
      "and: O\n",
      "diabetes: O\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "Predicted Named Entities:\n",
      "He: O\n",
      "presented: O\n",
      "to: O\n",
      "the: O\n",
      "emergency: O\n",
      "department: O\n",
      "with: O\n",
      "complaints: O\n",
      "of: O\n",
      "chest: O\n",
      "pain: O\n",
      "shortness: O\n",
      "of: O\n",
      "breath: O\n",
      "and: O\n",
      "dizziness: O\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "Predicted Named Entities:\n",
      "The: O\n",
      "patient: O\n",
      "s: O\n",
      "blood: O\n",
      "pressure: O\n",
      "was: O\n",
      "180: O\n",
      "110: O\n",
      "mmHg: O\n",
      "and: O\n",
      "his: O\n",
      "heart: O\n",
      "rate: O\n",
      "was: O\n",
      "110: O\n",
      "beats: O\n",
      "per: O\n",
      "minute: O\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "Predicted Named Entities:\n",
      "A: O\n",
      "12: O\n",
      "lead: O\n",
      "electrocardiogram: O\n",
      "showed: O\n",
      "ST: O\n",
      "segment: O\n",
      "elevation: O\n",
      "in: O\n",
      "the: O\n",
      "anterior: O\n",
      "leads: O\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "Predicted Named Entities:\n",
      "The: O\n",
      "patient: O\n",
      "was: O\n",
      "diagnosed: O\n",
      "with: O\n",
      "an: O\n",
      "acute: O\n",
      "myocardial: O\n",
      "infarction: O\n",
      "and: O\n",
      "was: O\n",
      "immediately: O\n",
      "started: O\n",
      "on: O\n",
      "heparin: O\n",
      "and: O\n",
      "aspirin: O\n",
      "therapy: O\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "Predicted Named Entities:\n",
      "He: O\n",
      "underwent: O\n",
      "a: O\n",
      "cardiac: O\n",
      "catheterization: O\n",
      "and: O\n",
      "was: O\n",
      "found: O\n",
      "to: O\n",
      "have: O\n",
      "significant: O\n",
      "stenosis: O\n",
      "in: O\n",
      "the: O\n",
      "left: O\n",
      "anterior: O\n",
      "descending: O\n",
      "artery: O\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "Predicted Named Entities:\n",
      "He: O\n",
      "underwent: O\n",
      "percutaneous: O\n",
      "coronary: O\n",
      "intervention: O\n",
      "with: O\n",
      "stent: O\n",
      "placement: O\n",
      "and: O\n",
      "his: O\n",
      "symptoms: O\n",
      "improved: O\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "Predicted Named Entities:\n",
      "He: O\n",
      "was: O\n",
      "discharged: O\n",
      "home: O\n",
      "on: O\n",
      "aspirin: O\n",
      "clopidogrel: O\n",
      "atorvastatin: O\n",
      "and: O\n",
      "lisinopril: O\n"
     ]
    }
   ],
   "source": [
    "# Split text into sentences\n",
    "text = \"The patient is a 55-year-old male with a history of hypertension and diabetes. He presented to the emergency department with complaints of chest pain, shortness of breath, and dizziness. The patient's blood pressure was 180/110 mmHg and his heart rate was 110 beats per minute. A 12-lead electrocardiogram showed ST-segment elevation in the anterior leads. The patient was diagnosed with an acute myocardial infarction and was immediately started on heparin and aspirin therapy. He underwent a cardiac catheterization and was found to have significant stenosis in the left anterior descending artery. He underwent percutaneous coronary intervention with stent placement and his symptoms improved. He was discharged home on aspirin, clopidogrel, atorvastatin, and lisinopril.\"\n",
    "\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "# Process each sentence\n",
    "for sentence in sentences:\n",
    "    tokens = re.findall(r'\\b\\w+\\b', sentence)\n",
    "    sequence = tokenizer.texts_to_sequences([' '.join(clean_text(token) for token in tokens)])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "    # Make the prediction\n",
    "    prediction = model.predict(np.array(padded_sequence))\n",
    "\n",
    "    # Decode the prediction\n",
    "    predicted_labels = np.argmax(prediction, axis=-1)\n",
    "    predicted_labels = [id2label[i] for i in predicted_labels[0]]\n",
    "\n",
    "    # Print the predicted named entities\n",
    "    print(\"Predicted Named Entities:\")\n",
    "    for i in range(len(tokens)):\n",
    "        print(f\"{tokens[i]}: {''.join(predicted_labels[i])}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 12ms/step\n",
      "Predicted Named Entities:\n",
      "The: O\n",
      "patient: O\n",
      "is: O\n",
      "a: O\n",
      "55-year-old: O\n",
      "male: O\n",
      "with: B-Clinical_event\n",
      "a: B-Clinical_event\n",
      "history: O\n",
      "of: O\n",
      "hypertension: O\n",
      "and: I-History\n",
      "diabetes.: I-History\n",
      "He: I-History\n",
      "presented: I-History\n",
      "to: I-History\n",
      "the: B-Clinical_event\n",
      "emergency: O\n",
      "department: O\n",
      "with: O\n",
      "complaints: O\n",
      "of: O\n",
      "chest: O\n",
      "pain,: O\n",
      "shortness: B-Biological_structure\n",
      "of: B-Sign_symptom\n",
      "breath,: B-Sign_symptom\n",
      "and: O\n",
      "dizziness.: I-Sign_symptom\n",
      "The: O\n",
      "patient's: B-Sign_symptom\n",
      "blood: O\n",
      "pressure: O\n",
      "was: B-Diagnostic_procedure\n",
      "180/110: I-Diagnostic_procedure\n",
      "mmHg: O\n",
      "and: B-Lab_value\n",
      "his: B-Lab_value\n",
      "heart: B-Lab_value\n",
      "rate: O\n",
      "was: O\n",
      "110: B-Diagnostic_procedure\n",
      "beats: I-Diagnostic_procedure\n",
      "per: O\n",
      "minute.: B-Lab_value\n",
      "A: I-Lab_value\n",
      "12-lead: I-Lab_value\n",
      "electrocardiogram: I-Lab_value\n",
      "showed: O\n",
      "ST-segment: O\n",
      "elevation: B-Diagnostic_procedure\n",
      "in: B-Diagnostic_procedure\n",
      "the: O\n",
      "anterior: O\n",
      "leads.: B-Biological_structure\n",
      "The: I-Sign_symptom\n",
      "patient: O\n",
      "was: O\n",
      "diagnosed: B-Biological_structure\n",
      "with: O\n",
      "an: O\n",
      "acute: O\n",
      "myocardial: O\n",
      "infarction: O\n",
      "and: O\n",
      "was: O\n",
      "immediately: O\n",
      "started: B-Disease_disorder\n",
      "on: I-Disease_disorder\n",
      "heparin: O\n",
      "and: O\n",
      "aspirin: O\n",
      "therapy.: O\n",
      "He: O\n",
      "underwent: B-Medication\n",
      "a: O\n",
      "cardiac: B-Medication\n",
      "catheterization: B-Medication\n",
      "and: O\n",
      "was: O\n",
      "found: O\n",
      "to: B-Medication\n",
      "have: I-Diagnostic_procedure\n",
      "significant: O\n",
      "stenosis: O\n",
      "in: O\n",
      "the: O\n",
      "left: O\n",
      "anterior: O\n",
      "descending: I-Dosage\n",
      "artery.: O\n",
      "He: O\n",
      "underwent: B-Biological_structure\n",
      "percutaneous: B-Biological_structure\n",
      "coronary: I-Biological_structure\n",
      "intervention: I-Biological_structure\n",
      "with: O\n",
      "stent: O\n",
      "placement: O\n",
      "and: I-Biological_structure\n",
      "his: I-Biological_structure\n",
      "symptoms: O\n",
      "improved.: B-Disease_disorder\n",
      "He: O\n",
      "was: O\n",
      "discharged: O\n",
      "home: B-Sign_symptom\n",
      "on: B-Sign_symptom\n",
      "aspirin,: O\n",
      "clopidogrel,: O\n",
      "atorvastatin,: B-Clinical_event\n",
      "and: O\n",
      "lisinopril.: O\n"
     ]
    }
   ],
   "source": [
    "text = \"The patient is a 55-year-old male with a history of hypertension and diabetes. He presented to the emergency department with complaints of chest pain, shortness of breath, and dizziness. The patient's blood pressure was 180/110 mmHg and his heart rate was 110 beats per minute. A 12-lead electrocardiogram showed ST-segment elevation in the anterior leads. The patient was diagnosed with an acute myocardial infarction and was immediately started on heparin and aspirin therapy. He underwent a cardiac catheterization and was found to have significant stenosis in the left anterior descending artery. He underwent percutaneous coronary intervention with stent placement and his symptoms improved. He was discharged home on aspirin, clopidogrel, atorvastatin, and lisinopril.\"\n",
    "\n",
    "sequence = tokenizer.texts_to_sequences([text])\n",
    "padded_sequence = pad_sequences(sequence, maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "# Make the prediction\n",
    "prediction = model.predict(np.array(padded_sequence))\n",
    "\n",
    "# Decode the prediction\n",
    "predicted_labels = np.argmax(prediction, axis=-1)\n",
    "predicted_labels = [id2label[i] for i in predicted_labels[0]]\n",
    "\n",
    "# Print the predicted named entities\n",
    "print(\"Predicted Named Entities:\")\n",
    "for i in range(len(text.split())):\n",
    "    print(f\"{text.split()[i]}: {''.join(predicted_labels[i])}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow_addons.text import crf_log_likelihood, crf_decode\n",
    "\n",
    "class CRF(tf.keras.Model):\n",
    "    def __init__(self, num_labels):\n",
    "        super(CRF, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.transition_params = tf.Variable(tf.random.normal(shape=(num_labels, num_labels)))\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        input_features, label_ids, input_mask = inputs\n",
    "        sequence_lengths = tf.reduce_sum(input_mask, axis=1)\n",
    "        logits = tf.keras.layers.Dense(self.num_labels)(input_features)\n",
    "        log_likelihood, self.transition_params = crf_log_likelihood(logits, label_ids, sequence_lengths,\n",
    "                                                                    transition_params=self.transition_params)\n",
    "        loss = -tf.reduce_mean(log_likelihood)\n",
    "        pred_ids, _ = crf_decode(logits, self.transition_params, sequence_lengths)\n",
    "        return loss, pred_ids\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "outputs": [
    {
     "data": {
      "text/plain": "34"
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_CLASSES"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "outputs": [
    {
     "ename": "OperatorNotAllowedInGraphError",
     "evalue": "Exception encountered when calling layer \"crf_2\" (type CRF).\n\nin user code:\n\n    File \"/var/folders/1k/l9m7dlqd1knbl3m543_55s4h0000gn/T/ipykernel_8978/4124767977.py\", line 12, in call  *\n        input_features, label_ids, input_mask = inputs\n\n    OperatorNotAllowedInGraphError: Iterating over a symbolic `tf.Tensor` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.\n\n\nCall arguments received by layer \"crf_2\" (type CRF):\n  • inputs=tf.Tensor(shape=(None, 128, 34), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOperatorNotAllowedInGraphError\u001B[0m            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[174], line 8\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_addons\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtext\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m crf_log_likelihood, crf_decode\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# Define the CRF model architecture\u001B[39;00m\n\u001B[0;32m----> 8\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mSequential\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlayers\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mInput\u001B[49m\u001B[43m(\u001B[49m\u001B[43mshape\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mMAX_LENGTH\u001B[49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mint32\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlayers\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mEmbedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_dim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mVOCAB_SIZE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_dim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mEMBEDDING_DIM\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mMAX_LENGTH\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlayers\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mBidirectional\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlayers\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mLSTM\u001B[49m\u001B[43m(\u001B[49m\u001B[43munits\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m64\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_sequences\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlayers\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDense\u001B[49m\u001B[43m(\u001B[49m\u001B[43mNUM_CLASSES\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[43m    \u001B[49m\u001B[43mCRF\u001B[49m\u001B[43m(\u001B[49m\u001B[43mNUM_CLASSES\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m# Compile the model\u001B[39;00m\n\u001B[1;32m     17\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m Adam(lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.001\u001B[39m)\n",
      "File \u001B[0;32m~/dev/projects/NER-medical-text/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py:587\u001B[0m, in \u001B[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    585\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_self_setattr_tracking \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[1;32m    586\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 587\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[43mmethod\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    588\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    589\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_self_setattr_tracking \u001B[38;5;241m=\u001B[39m previous_value  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n",
      "File \u001B[0;32m~/dev/projects/NER-medical-text/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     65\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[1;32m     66\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m---> 67\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m     68\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     69\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m~/dev/projects/NER-medical-text/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py:1127\u001B[0m, in \u001B[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m   1125\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint:disable=broad-except\u001B[39;00m\n\u001B[1;32m   1126\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(e, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mag_error_metadata\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m-> 1127\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mag_error_metadata\u001B[38;5;241m.\u001B[39mto_exception(e)\n\u001B[1;32m   1128\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1129\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[0;31mOperatorNotAllowedInGraphError\u001B[0m: Exception encountered when calling layer \"crf_2\" (type CRF).\n\nin user code:\n\n    File \"/var/folders/1k/l9m7dlqd1knbl3m543_55s4h0000gn/T/ipykernel_8978/4124767977.py\", line 12, in call  *\n        input_features, label_ids, input_mask = inputs\n\n    OperatorNotAllowedInGraphError: Iterating over a symbolic `tf.Tensor` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.\n\n\nCall arguments received by layer \"crf_2\" (type CRF):\n  • inputs=tf.Tensor(shape=(None, 128, 34), dtype=float32)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow_addons.text import crf_log_likelihood, crf_decode\n",
    "\n",
    "# Define the CRF model architecture\n",
    "model = Sequential([\n",
    "    layers.Input(shape=(MAX_LENGTH,), dtype=tf.int32),\n",
    "    layers.Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_DIM, input_length=MAX_LENGTH),\n",
    "    layers.Bidirectional(layers.LSTM(units=64, return_sequences=True)),\n",
    "    layers.Dense(NUM_CLASSES),\n",
    "    CRF(NUM_CLASSES)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(lr=0.001)\n",
    "model.compile(optimizer=optimizer, loss=crf_log_likelihood, metrics=[crf_decode])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"crf_4\" (type CRF).\n\nin user code:\n\n    File \"/var/folders/1k/l9m7dlqd1knbl3m543_55s4h0000gn/T/ipykernel_8978/577521863.py\", line 16, in call  *\n        path, _ = crf_decode(logits, self.transition_params, tf.reduce_sum(tf.cast(inputs != 0, dtype=tf.int32), axis=-1))\n    File \"/Users/ajaykarthicksenthilkumar/dev/projects/NER-medical-text/lib/python3.8/site-packages/tensorflow_addons/text/crf.py\", line 570, in _multi_seq_fn  *\n        backpointers = tf.reverse_sequence(\n\n    ValueError: Shape must be rank 1 but is rank 2 for '{{node cond/ReverseSequence}} = ReverseSequence[T=DT_INT32, Tlen=DT_INT32, batch_dim=0, seq_dim=1](cond/rnn/transpose_2, cond/Maximum)' with input shapes: [?,?,34], [?,?].\n\n\nCall arguments received by layer \"crf_4\" (type CRF):\n  • inputs=tf.Tensor(shape=(None, 128, 34), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[176], line 21\u001B[0m\n\u001B[1;32m     17\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m path\n\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m# Define the CRF model architecture\u001B[39;00m\n\u001B[0;32m---> 21\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mSequential\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\n\u001B[1;32m     22\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlayers\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mInput\u001B[49m\u001B[43m(\u001B[49m\u001B[43mshape\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mMAX_LENGTH\u001B[49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mint32\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     23\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlayers\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mEmbedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_dim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mVOCAB_SIZE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_dim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mEMBEDDING_DIM\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mMAX_LENGTH\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     24\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlayers\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mBidirectional\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlayers\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mLSTM\u001B[49m\u001B[43m(\u001B[49m\u001B[43munits\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m64\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_sequences\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     25\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlayers\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDense\u001B[49m\u001B[43m(\u001B[49m\u001B[43mNUM_CLASSES\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     26\u001B[0m \u001B[43m    \u001B[49m\u001B[43mCRF\u001B[49m\u001B[43m(\u001B[49m\u001B[43mNUM_CLASSES\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     27\u001B[0m \u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;66;03m# Compile the model\u001B[39;00m\n\u001B[1;32m     30\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m Adam(lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.001\u001B[39m)\n",
      "File \u001B[0;32m~/dev/projects/NER-medical-text/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py:587\u001B[0m, in \u001B[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    585\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_self_setattr_tracking \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[1;32m    586\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 587\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[43mmethod\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    588\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    589\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_self_setattr_tracking \u001B[38;5;241m=\u001B[39m previous_value  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n",
      "File \u001B[0;32m~/dev/projects/NER-medical-text/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     65\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[1;32m     66\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m---> 67\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m     68\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     69\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m/var/folders/1k/l9m7dlqd1knbl3m543_55s4h0000gn/T/__autograph_generated_fileb61cni15.py:11\u001B[0m, in \u001B[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001B[0;34m(self, inputs)\u001B[0m\n\u001B[1;32m      9\u001B[0m retval_ \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mUndefinedReturnValue()\n\u001B[1;32m     10\u001B[0m logits \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mld(inputs)\n\u001B[0;32m---> 11\u001B[0m (path, _) \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(crf_decode), (ag__\u001B[38;5;241m.\u001B[39mld(logits), ag__\u001B[38;5;241m.\u001B[39mld(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39mtransition_params, ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(tf)\u001B[38;5;241m.\u001B[39mreduce_sum, (ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(tf)\u001B[38;5;241m.\u001B[39mcast, ((ag__\u001B[38;5;241m.\u001B[39mld(inputs) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m),), \u001B[38;5;28mdict\u001B[39m(dtype\u001B[38;5;241m=\u001B[39mag__\u001B[38;5;241m.\u001B[39mld(tf)\u001B[38;5;241m.\u001B[39mint32), fscope),), \u001B[38;5;28mdict\u001B[39m(axis\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m)), fscope)), \u001B[38;5;28;01mNone\u001B[39;00m, fscope)\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     13\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m/var/folders/1k/l9m7dlqd1knbl3m543_55s4h0000gn/T/__autograph_generated_filesixjip2b.py:111\u001B[0m, in \u001B[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__crf_decode\u001B[0;34m(potentials, transition_params, sequence_length)\u001B[0m\n\u001B[1;32m    109\u001B[0m         do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    110\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[0;32m--> 111\u001B[0m ag__\u001B[38;5;241m.\u001B[39mif_stmt((ag__\u001B[38;5;241m.\u001B[39mld(potentials)\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m), if_body_2, else_body_2, get_state_2, set_state_2, (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdo_return\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mretval_\u001B[39m\u001B[38;5;124m'\u001B[39m), \u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m    112\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m fscope\u001B[38;5;241m.\u001B[39mret(retval_, do_return)\n",
      "File \u001B[0;32m/var/folders/1k/l9m7dlqd1knbl3m543_55s4h0000gn/T/__autograph_generated_filesixjip2b.py:107\u001B[0m, in \u001B[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__crf_decode.<locals>.else_body_2\u001B[0;34m()\u001B[0m\n\u001B[1;32m    105\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    106\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m--> 107\u001B[0m     retval_ \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(tf)\u001B[38;5;241m.\u001B[39mcond, (ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(tf)\u001B[38;5;241m.\u001B[39mequal, (ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(tf)\u001B[38;5;241m.\u001B[39mshape, (ag__\u001B[38;5;241m.\u001B[39mld(potentials),), \u001B[38;5;28;01mNone\u001B[39;00m, fscope)[\u001B[38;5;241m1\u001B[39m], \u001B[38;5;241m1\u001B[39m), \u001B[38;5;28;01mNone\u001B[39;00m, fscope), ag__\u001B[38;5;241m.\u001B[39mld(_single_seq_fn), ag__\u001B[38;5;241m.\u001B[39mld(_multi_seq_fn)), \u001B[38;5;28;01mNone\u001B[39;00m, fscope)\n\u001B[1;32m    108\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[1;32m    109\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m/var/folders/1k/l9m7dlqd1knbl3m543_55s4h0000gn/T/__autograph_generated_filesixjip2b.py:51\u001B[0m, in \u001B[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__crf_decode.<locals>._multi_seq_fn\u001B[0;34m()\u001B[0m\n\u001B[1;32m     49\u001B[0m sequence_length_less_one \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(tf)\u001B[38;5;241m.\u001B[39mmaximum, (ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(tf)\u001B[38;5;241m.\u001B[39mconstant, (\u001B[38;5;241m0\u001B[39m,), \u001B[38;5;28mdict\u001B[39m(dtype\u001B[38;5;241m=\u001B[39mag__\u001B[38;5;241m.\u001B[39mld(tf)\u001B[38;5;241m.\u001B[39mint32), fscope_2), (ag__\u001B[38;5;241m.\u001B[39mld(sequence_length) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m)), \u001B[38;5;28;01mNone\u001B[39;00m, fscope_2)\n\u001B[1;32m     50\u001B[0m (backpointers, last_score) \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(crf_decode_forward), (ag__\u001B[38;5;241m.\u001B[39mld(inputs), ag__\u001B[38;5;241m.\u001B[39mld(initial_state), ag__\u001B[38;5;241m.\u001B[39mld(transition_params), ag__\u001B[38;5;241m.\u001B[39mld(sequence_length_less_one)), \u001B[38;5;28;01mNone\u001B[39;00m, fscope_2)\n\u001B[0;32m---> 51\u001B[0m backpointers \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(tf)\u001B[38;5;241m.\u001B[39mreverse_sequence, (ag__\u001B[38;5;241m.\u001B[39mld(backpointers), ag__\u001B[38;5;241m.\u001B[39mld(sequence_length_less_one)), \u001B[38;5;28mdict\u001B[39m(seq_axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m), fscope_2)\n\u001B[1;32m     52\u001B[0m initial_state \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(tf)\u001B[38;5;241m.\u001B[39mcast, (ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(tf)\u001B[38;5;241m.\u001B[39margmax, (ag__\u001B[38;5;241m.\u001B[39mld(last_score),), \u001B[38;5;28mdict\u001B[39m(axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m), fscope_2),), \u001B[38;5;28mdict\u001B[39m(dtype\u001B[38;5;241m=\u001B[39mag__\u001B[38;5;241m.\u001B[39mld(tf)\u001B[38;5;241m.\u001B[39mint32), fscope_2)\n\u001B[1;32m     53\u001B[0m initial_state \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(tf)\u001B[38;5;241m.\u001B[39mexpand_dims, (ag__\u001B[38;5;241m.\u001B[39mld(initial_state),), \u001B[38;5;28mdict\u001B[39m(axis\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m)), fscope_2)\n",
      "\u001B[0;31mValueError\u001B[0m: Exception encountered when calling layer \"crf_4\" (type CRF).\n\nin user code:\n\n    File \"/var/folders/1k/l9m7dlqd1knbl3m543_55s4h0000gn/T/ipykernel_8978/577521863.py\", line 16, in call  *\n        path, _ = crf_decode(logits, self.transition_params, tf.reduce_sum(tf.cast(inputs != 0, dtype=tf.int32), axis=-1))\n    File \"/Users/ajaykarthicksenthilkumar/dev/projects/NER-medical-text/lib/python3.8/site-packages/tensorflow_addons/text/crf.py\", line 570, in _multi_seq_fn  *\n        backpointers = tf.reverse_sequence(\n\n    ValueError: Shape must be rank 1 but is rank 2 for '{{node cond/ReverseSequence}} = ReverseSequence[T=DT_INT32, Tlen=DT_INT32, batch_dim=0, seq_dim=1](cond/rnn/transpose_2, cond/Maximum)' with input shapes: [?,?,34], [?,?].\n\n\nCall arguments received by layer \"crf_4\" (type CRF):\n  • inputs=tf.Tensor(shape=(None, 128, 34), dtype=float32)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow_addons.text import crf_log_likelihood, crf_decode\n",
    "\n",
    "class CRF(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_labels):\n",
    "        super(CRF, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.transition_params = self.add_weight(shape=(num_labels, num_labels), initializer='glorot_uniform')\n",
    "\n",
    "    @tf.function(input_signature=[tf.TensorSpec(shape=(None, None, None), dtype=tf.float32)])\n",
    "    def call(self, inputs):\n",
    "        logits = inputs\n",
    "        path, _ = crf_decode(logits, self.transition_params, tf.reduce_sum(tf.cast(inputs != 0, dtype=tf.int32), axis=-1))\n",
    "        return path\n",
    "\n",
    "\n",
    "# Define the CRF model architecture\n",
    "model = Sequential([\n",
    "    layers.Input(shape=(MAX_LENGTH,), dtype=tf.int32),\n",
    "    layers.Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_DIM, input_length=MAX_LENGTH),\n",
    "    layers.Bidirectional(layers.LSTM(units=64, return_sequences=True)),\n",
    "    layers.Dense(NUM_CLASSES),\n",
    "    CRF(NUM_CLASSES)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(lr=0.001)\n",
    "\n",
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss, _ = model(x)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Train the model\n",
    "for epoch in range(10):\n",
    "    epoch_loss = 0\n",
    "    for x, y in train_dataset:\n",
    "        loss = train_step(x, y)\n",
    "        epoch_loss += loss\n",
    "    print(f\"Epoch {epoch+1} loss: {epoch_loss}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
