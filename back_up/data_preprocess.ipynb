{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "from typing import List, Tuple, Dict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_text_file(file_path: str) -> str:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def tokenize_text(text: str) -> List[str]:\n",
    "    # Tokenize the text into a list of words\n",
    "    tokens = []\n",
    "    for sentence in text.split('\\n'):\n",
    "        for word in sentence.split():\n",
    "            # Remove trailing punctuation marks from the word\n",
    "            while word and word[-1] in string.punctuation:\n",
    "                word = word[:-1]\n",
    "            tokens.append(word)\n",
    "    return tokens\n",
    "    # tokens = [word for sentence in text.split('\\n') for word in sentence.split()]\n",
    "    # return tokens"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def get_start_end_range_to_token_index(text, tokens, entity_ranges):\n",
    "    # Initialize a dictionary to map each (start, end) range to the corresponding token indices\n",
    "    start_end_range_to_token_index = {}\n",
    "    # Keep track of the current position in the text\n",
    "    current_pos = 0\n",
    "    # Iterate over each token in the tokens list\n",
    "    for i in range(len(tokens)):\n",
    "        # Calculate the starting position of the token\n",
    "        token_start = text.find(tokens[i], current_pos)\n",
    "        token_end = token_start + len(tokens[i])\n",
    "        # Update the current position in the text\n",
    "        current_pos = token_end\n",
    "        # Check if the current token is inside any of the entity ranges\n",
    "        for label, start, end in entity_ranges:\n",
    "            if start <= token_start and end >= token_end:\n",
    "                # If the (start, end) range is not already in the dictionary, add it with an empty list\n",
    "                if (start, end) not in start_end_range_to_token_index:\n",
    "                    start_end_range_to_token_index[(start, end)] = []\n",
    "                # Add the index of the token to the list corresponding to the (start, end) range in the dictionary\n",
    "                start_end_range_to_token_index[(start, end)].append(i)\n",
    "    return start_end_range_to_token_index\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def read_annotation_file(file_path: str, selected_entities: List[str]) -> List[Tuple[str, List[Tuple[int,int]]]]:\n",
    "    entity_ranges = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            fields = line.strip().split('\\t')\n",
    "            # Get the tag and its starting and ending positions\n",
    "            if fields[0].startswith('T'):\n",
    "                entity_with_range, word = fields[1], fields[2]\n",
    "                label = entity_with_range.split()[0]\n",
    "                if label in selected_entities:\n",
    "                    ranges = [\n",
    "                        (\n",
    "                            int(start_end.split()[0]),\n",
    "                            int(start_end.split()[1])\n",
    "                        )\n",
    "                        for start_end in ' '.join(entity_with_range.split()[1:]).split(';')\n",
    "                    ]\n",
    "                    entity_ranges.append((label, ranges))\n",
    "    # Sort the entity ranges based on start and end\n",
    "    entity_ranges = sorted(entity_ranges, key=lambda x: (x[1][0][0], x[1][0][1]))\n",
    "    return entity_ranges"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def convert_ann_to_bio(input_dir: str, output_dir: str, selected_entities: List[str]):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    for file_name in os.listdir(input_dir):\n",
    "        if file_name.endswith('.txt'):\n",
    "            # Read the corresponding txt file\n",
    "            text = read_text_file(os.path.join(input_dir, file_name))\n",
    "\n",
    "            # Find the corresponding ann file\n",
    "            ann_file = os.path.join(input_dir, file_name.replace('.txt', '.ann'))\n",
    "\n",
    "            # Tokenize the text\n",
    "            tokens = tokenize_text(text)\n",
    "\n",
    "            # Initialize a list to hold the BIO-formatted tags\n",
    "            bio_tags = ['O'] * len(tokens)\n",
    "\n",
    "            # Read the annotation file\n",
    "            entity_ranges = read_annotation_file(ann_file, selected_entities)\n",
    "            entity_ranges = [(name, *tup) for name, tup_list in entity_ranges for tup in tup_list]\n",
    "\n",
    "            start_end_2_idx = get_start_end_range_to_token_index(text, tokens, entity_ranges)\n",
    "\n",
    "            # Update the BIO tags\n",
    "            for label, start, end in entity_ranges:\n",
    "                # Get the list of token indices corresponding to the (start, end) range\n",
    "                token_indices = start_end_2_idx.get((start, end), [])\n",
    "                # Assign the BIO tags to each token index in the range\n",
    "                for i in token_indices:\n",
    "                    if i == token_indices[0]:\n",
    "                        bio_tags[i] = 'B-' + label\n",
    "                    else:\n",
    "                        bio_tags[i] = 'I-' + label\n",
    "\n",
    "            # Write the BIO tags to a new file\n",
    "            with open(os.path.join(output_dir, file_name.replace('.txt', '.bio')), 'w', encoding='utf-8') as f:\n",
    "                sentence_start_index = 0\n",
    "                for sentence in text.split('\\n'):\n",
    "                    sentence_tokens = sentence.split()\n",
    "                    sentence_length = len(sentence_tokens)\n",
    "                    sentence_end_index = sentence_start_index + sentence_length\n",
    "                    for i in range(sentence_start_index, sentence_end_index):\n",
    "                        f.write(sentence_tokens[i - sentence_start_index] + '\\t' + bio_tags[i] + '\\n')\n",
    "                    f.write('\\n')\n",
    "                    sentence_start_index = sentence_end_index\n",
    "\n",
    "    print(\"Conversion completed successfully.\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "selected_entities = ['Age', 'Biological_attribute', 'Biological_structure', 'Clinical_event', 'Diagnostic_procedure', 'Disease_disorder', 'Dosage', 'Family_history', 'Height', 'History', 'Lab_value', 'Mass', 'Medication', 'Sex', 'Sign_symptom', 'Therapeutic_procedure', 'Weight']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion completed successfully.\n"
     ]
    }
   ],
   "source": [
    "convert_ann_to_bio('./data/MACCROBAT', './data/BIO_FILES', selected_entities)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
