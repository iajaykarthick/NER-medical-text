{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "75a9cab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7d42d817",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ajaykarthicksenthilkumar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "#nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "STOP_WORDS = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b8516e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_files_dir = '../data/bio_data_files'\n",
    "bio_files = [os.path.join(bio_files_dir, f) for f in os.listdir('../data/bio_data_files') if f.endswith('.bio')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cebfd940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of .bio files is 200\n"
     ]
    }
   ],
   "source": [
    "print(f\"The number of .bio files is {len(bio_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe2b598",
   "metadata": {},
   "source": [
    "## Check if any of the stopwords contain B-tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4d9f940a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for bio_file in bio_files:\n",
    "    with open(bio_file, \"r\", encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip() == '':\n",
    "                continue\n",
    "            word, tag = line.strip().split('\\t')\n",
    "            if word in STOP_WORDS and tag.startswith('B'):\n",
    "                print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbe4fa1",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "73f0e53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_word(word):\n",
    "    \n",
    "    # remove non-alphanumeric characters and extra whitespaces\n",
    "    word = re.sub(r'[^\\w\\s]','',word)\n",
    "    word = re.sub(r'\\s+',' ',word)\n",
    "    \n",
    "    # convert to lowercase\n",
    "    word = word.lower()\n",
    "    \n",
    "    if word not in STOP_WORDS:\n",
    "        return word\n",
    "    \n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "09853edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained Spacy model and set the stop words\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def clean_word(word):\n",
    "    # remove non-alphanumeric characters and extra whitespaces\n",
    "    word = re.sub(r'[^\\w\\s]','',word)\n",
    "    word = re.sub(r'\\s+',' ',word)\n",
    "    \n",
    "    # convert to lowercase\n",
    "    word = word.lower()\n",
    "\n",
    "    # lemmatize the word\n",
    "    lemma = nlp(word)[0].lemma_\n",
    "    \n",
    "    # check if the lemma is a stop word\n",
    "    if lemma not in STOP_WORDS:\n",
    "        return lemma\n",
    "    \n",
    "    return ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "141f77d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data_from_file(bio_file):\n",
    "    \"\"\"\n",
    "    Reads a file in BIO format (one token per line, with tab-separated word and tag),\n",
    "    and extracts the sentences and labels as lists of lists. Each inner list represents\n",
    "    a sentence, and contains the words of the sentence in order. Each corresponding inner\n",
    "    list in the 'labels' list contains the BIO tags for the words in the corresponding\n",
    "    sentence, in the same order.\n",
    "    \n",
    "    Args:\n",
    "    - bio_file (str): the path to the BioNLP file to read\n",
    "    \n",
    "    Returns:\n",
    "    - A tuple containing:\n",
    "        - sentences (List[List[str]]): a list of lists, where each inner list represents\n",
    "          a sentence and contains the words of the sentence in order\n",
    "        - labels (List[List[str]]): a list of lists, where each inner list corresponds\n",
    "          to a sentence in the 'sentences' list and contains the BIO tags for the words\n",
    "          in the corresponding sentence, in the same order.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    \n",
    "    with open(bio_file, \"r\", encoding='utf-8') as f:\n",
    "        \n",
    "        current_sentences = []\n",
    "        current_labels = []\n",
    "        \n",
    "        for line in f:\n",
    "            \n",
    "            if line.strip() == '':\n",
    "                # If we encounter a blank line, it means we've reached the end of a sentence\n",
    "                if len(current_sentences) > 0:\n",
    "                    \n",
    "                    # Add the current sentence and labels to the list\n",
    "                    sentences.append(current_sentences)\n",
    "                    labels.append(current_labels)\n",
    "                    \n",
    "                    # Reset the current sentence and labels lists\n",
    "                    current_sentences = []\n",
    "                    current_labels = []\n",
    "                    continue\n",
    "                    \n",
    "            word, tag = line.strip().split('\\t')\n",
    "            word = clean_word(word)\n",
    "            \n",
    "            if word.strip():\n",
    "                current_sentences.append(word)\n",
    "                \n",
    "                if len(current_labels) > 0:\n",
    "                    if tag[2:] == current_labels[-1][2:] and tag[:2] == \"B-\":\n",
    "                        tag = f\"I-{tag[2:]}\"\n",
    "                current_labels.append(tag)\n",
    "        \n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fd844f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_bio_files(bio_files):\n",
    "    \n",
    "    sentences = []\n",
    "    labels = []\n",
    "    \n",
    "    for idx, bio_file in enumerate(bio_files):\n",
    "        \n",
    "        curr_sentences, curr_labels = parse_data_from_file(bio_file)\n",
    "        \n",
    "        if len(curr_sentences) > 0:\n",
    "            sentences.extend(curr_sentences)\n",
    "            labels.extend(curr_labels)\n",
    "            \n",
    "        if (idx+1) % 20 == 0:\n",
    "            print(f'{idx+1} completed')\n",
    "\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "45fd1f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 completed\n",
      "40 completed\n",
      "60 completed\n",
      "80 completed\n",
      "100 completed\n",
      "120 completed\n",
      "140 completed\n",
      "160 completed\n",
      "180 completed\n",
      "200 completed\n"
     ]
    }
   ],
   "source": [
    "sentences, labels = parse_bio_files(bio_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b53eabb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset contains 4341 examples\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset contains {len(sentences)} examples\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46342054",
   "metadata": {},
   "source": [
    "## Shuffle the sentences and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "56e01b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = list(zip(sentences, labels))\n",
    "random.shuffle(combined)\n",
    "sentences[:], labels[:] = zip(*combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722ff4a8",
   "metadata": {},
   "source": [
    "## Train Test Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "30bb6251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training, validation, and test sets\n",
    "\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "num_sentences = len(sentences)\n",
    "num_train = int(num_sentences * (1 - TEST_SIZE - 0.1))\n",
    "num_valid = int(num_sentences * 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0c1d5432",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = sentences[:num_train]\n",
    "train_labels = labels[:num_train]\n",
    "\n",
    "valid_sentences = sentences[num_train:num_train+num_valid]\n",
    "valid_labels = labels[num_train:num_train+num_valid]\n",
    "\n",
    "test_sentences = sentences[num_train+num_valid:]\n",
    "test_labels = labels[num_train+num_valid:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d413c70",
   "metadata": {},
   "source": [
    "## Tokenization - Sequences and padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "de6ea2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = set(element for sublist in labels for element in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9e0dbaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_index = {label: id+1 for id, label in enumerate(sorted(unique_labels))}\n",
    "index_to_label = {id: label for label, id in label_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9ba093e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the new label and ID to the dictionaries\n",
    "label_to_index['<PAD>'] = 0\n",
    "index_to_label[0] = '<PAD>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c4f7383a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = len(index_to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1351ada3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'B-ACT',\n",
       " 2: 'B-ADM',\n",
       " 3: 'B-AGE',\n",
       " 4: 'B-ARA',\n",
       " 5: 'B-BAT',\n",
       " 6: 'B-BST',\n",
       " 7: 'B-CLE',\n",
       " 8: 'B-COL',\n",
       " 9: 'B-COR',\n",
       " 10: 'B-DAT',\n",
       " 11: 'B-DET',\n",
       " 12: 'B-DIA',\n",
       " 13: 'B-DIS',\n",
       " 14: 'B-DOS',\n",
       " 15: 'B-DUR',\n",
       " 16: 'B-FAM',\n",
       " 17: 'B-FRE',\n",
       " 18: 'B-HEI',\n",
       " 19: 'B-HIS',\n",
       " 20: 'B-LAB',\n",
       " 21: 'B-MAS',\n",
       " 22: 'B-MED',\n",
       " 23: 'B-NBL',\n",
       " 24: 'B-OCC',\n",
       " 25: 'B-OTE',\n",
       " 26: 'B-OTH',\n",
       " 27: 'B-OUT',\n",
       " 28: 'B-PER',\n",
       " 29: 'B-QUC',\n",
       " 30: 'B-SEV',\n",
       " 31: 'B-SEX',\n",
       " 32: 'B-SHA',\n",
       " 33: 'B-SIG',\n",
       " 34: 'B-SUB',\n",
       " 35: 'B-TEX',\n",
       " 36: 'B-THP',\n",
       " 37: 'B-TIM',\n",
       " 38: 'B-VOL',\n",
       " 39: 'B-WEI',\n",
       " 40: 'I-ACT',\n",
       " 41: 'I-ADM',\n",
       " 42: 'I-AGE',\n",
       " 43: 'I-ARA',\n",
       " 44: 'I-BAT',\n",
       " 45: 'I-BST',\n",
       " 46: 'I-CLE',\n",
       " 47: 'I-COL',\n",
       " 48: 'I-COR',\n",
       " 49: 'I-DAT',\n",
       " 50: 'I-DET',\n",
       " 51: 'I-DIA',\n",
       " 52: 'I-DIS',\n",
       " 53: 'I-DOS',\n",
       " 54: 'I-DUR',\n",
       " 55: 'I-FAM',\n",
       " 56: 'I-FRE',\n",
       " 57: 'I-HEI',\n",
       " 58: 'I-HIS',\n",
       " 59: 'I-LAB',\n",
       " 60: 'I-MAS',\n",
       " 61: 'I-MED',\n",
       " 62: 'I-NBL',\n",
       " 63: 'I-OCC',\n",
       " 64: 'I-OTE',\n",
       " 65: 'I-OTH',\n",
       " 66: 'I-OUT',\n",
       " 67: 'I-PER',\n",
       " 68: 'I-QUC',\n",
       " 69: 'I-SEV',\n",
       " 70: 'I-SHA',\n",
       " 71: 'I-SIG',\n",
       " 72: 'I-SUB',\n",
       " 73: 'I-TEX',\n",
       " 74: 'I-THP',\n",
       " 75: 'I-TIM',\n",
       " 76: 'I-VOL',\n",
       " 77: 'I-WEI',\n",
       " 78: 'O',\n",
       " 0: '<PAD>'}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "91f9e4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 100\n",
    "\n",
    "# Convert the labels to indices\n",
    "train_labels_indices = [[label_to_index[label] for label in labels] for labels in train_labels]\n",
    "valid_labels_indices = [[label_to_index[label] for label in labels] for labels in valid_labels]\n",
    "test_labels_indices = [[label_to_index[label] for label in labels] for labels in test_labels]\n",
    "\n",
    "# Pad the sequences to a fixed length with the new label ('<PAD>' value = 0)\n",
    "train_labels_padded = pad_sequences(train_labels_indices, maxlen=MAX_LENGTH, padding='post', value=0)\n",
    "valid_labels_padded = pad_sequences(valid_labels_indices, maxlen=MAX_LENGTH, padding='post', value=0)\n",
    "test_labels_padded = pad_sequences(test_labels_indices, maxlen=MAX_LENGTH, padding='post', value=0)\n",
    "\n",
    "\n",
    "# Convert the labels to categorical format\n",
    "train_labels_categorical = to_categorical(train_labels_padded, num_classes=NUM_CLASSES)\n",
    "valid_labels_categorical = to_categorical(valid_labels_padded, num_classes=NUM_CLASSES)\n",
    "test_labels_categorical = to_categorical(test_labels_padded, num_classes=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "89210aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 6860\n"
     ]
    }
   ],
   "source": [
    "# Convert the input sentences to sequences of word indices\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "\n",
    "tokenizer.word_index['<PAD>'] = 0\n",
    "tokenizer.index_word[0] = '<PAD>'\n",
    "\n",
    "# Calculate the vocabulary size\n",
    "VOCAB_SIZE = len(tokenizer.word_index)\n",
    "print(f\"Vocabulary size: {VOCAB_SIZE}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a5fc9b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the input sentences to sequences of word indices\n",
    "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "val_sequences = tokenizer.texts_to_sequences(valid_sentences)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
    "\n",
    "# oov_words_train = [word for sentence in train_sentences for word in sentence if word not in tokenizer.word_index]\n",
    "# oov_words_val = [word for sentence in valid_sentences for word in sentence if word not in tokenizer.word_index]\n",
    "# oov_words_test = [word for sentence in test_sentences for word in sentence if word not in tokenizer.word_index]\n",
    "\n",
    "# print(f\"Number of OOV words in train: {len(oov_words_train)}\")\n",
    "# print(f\"Number of OOV words in val: {len(oov_words_val)}\")\n",
    "# print(f\"Number of OOV words in test: {len(oov_words_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b1819b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad the sequences\n",
    "train_sequences_padded = pad_sequences(train_sequences, maxlen=MAX_LENGTH, padding='post', truncating='post')\n",
    "val_sequences_padded = pad_sequences(val_sequences, maxlen=MAX_LENGTH, padding='post', truncating='post')\n",
    "test_sequences_padded = pad_sequences(test_sequences, maxlen=MAX_LENGTH, padding='post', truncating='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fc940b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cfcdb497",
   "metadata": {},
   "source": [
    "###  save to a .npz file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "27d0e8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\n",
    "    '../data/data.npz',\n",
    "     train_sequences_padded=train_sequences_padded,\n",
    "     train_labels=train_labels_categorical,\n",
    "     val_sequences_padded=val_sequences_padded,\n",
    "     val_labels=valid_labels_categorical,\n",
    "     test_sequences_padded=test_sequences_padded,\n",
    "     test_labels=test_labels_categorical,\n",
    "     label_to_index=label_to_index,\n",
    "     index_to_label=index_to_label\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c25a8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2bf7f283",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4de8cb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
