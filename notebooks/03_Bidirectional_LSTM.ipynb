{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "bio_files_dir = '../data/BIO_FILES'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Defining Parameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 100000\n",
    "EMBEDDING_DIM = 128\n",
    "MAX_LENGTH = 200\n",
    "NUM_CLASSES = 35\n",
    "LSTM_UNITS = 64\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "TEST_SIZE = 0.2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Cleaning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Remove punctuation and convert to lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text).lower()\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocess and Load Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "\n",
    "def load_data(data_dir, max_length=None):\n",
    "\n",
    "    # Load all files in the data directory\n",
    "    all_files = os.listdir(data_dir)\n",
    "\n",
    "    # Filter only the files with the .bio extension\n",
    "    bio_files = [f for f in all_files if f.endswith('.bio')]\n",
    "\n",
    "    # Initialize lists to hold sentences and labels\n",
    "    sentences = []\n",
    "    labels = []\n",
    "\n",
    "    # Loop through each file and read the sentences and labels\n",
    "    for file in bio_files:\n",
    "        with open(os.path.join(data_dir, file), 'r', encoding='utf-8') as f:\n",
    "            current_sentences = []\n",
    "            current_labels = []\n",
    "            for line in f:\n",
    "                if line.strip() == '':\n",
    "                    # If we encounter a blank line, it means we've reached the end of a sentence\n",
    "                    if len(current_sentences) > 0:\n",
    "                        # Add the current sentence and labels to the list\n",
    "                        sentences.append(current_sentences)\n",
    "                        labels.append(current_labels)\n",
    "                        # Reset the current sentence and labels lists\n",
    "                        current_sentences = []\n",
    "                        current_labels = []\n",
    "                else:\n",
    "                    # Otherwise, split the line into its word and label components\n",
    "                    word, label = line.strip().split('\\t')\n",
    "                    current_sentences.append(clean_text(word))\n",
    "                    current_labels.append(label)\n",
    "\n",
    "    # Shuffle the sentences and labels\n",
    "    combined = list(zip(sentences, labels))\n",
    "    random.shuffle(combined)\n",
    "    sentences[:], labels[:] = zip(*combined)\n",
    "\n",
    "    # Split the data into training, validation, and test sets\n",
    "\n",
    "    num_sentences = len(sentences)\n",
    "    num_train = int(num_sentences * (1 - TEST_SIZE - 0.1))\n",
    "    num_valid = int(num_sentences * 0.1)\n",
    "\n",
    "    train_sentences = sentences[:num_train]\n",
    "    train_labels = labels[:num_train]\n",
    "\n",
    "    valid_sentences = sentences[num_train:num_train+num_valid]\n",
    "    valid_labels = labels[num_train:num_train+num_valid]\n",
    "\n",
    "    test_sentences = sentences[num_train+num_valid:]\n",
    "    test_labels = labels[num_train+num_valid:]\n",
    "\n",
    "    # Convert the labels to one-hot encoding\n",
    "    unique_labels = set(element for sublist in labels for element in sublist)\n",
    "    label_to_index = {label: id+1 for id, label in enumerate(sorted(unique_labels))}\n",
    "    index_to_label = {id: label for label, id in label_to_index.items()}\n",
    "\n",
    "    # Add the new label and ID to the dictionaries\n",
    "    label_to_index['<PAD>'] = 0\n",
    "    index_to_label[0] = '<PAD>'\n",
    "\n",
    "    num_classes = len(index_to_label) - 1\n",
    "\n",
    "    train_labels = [[label_to_index[label] for label in labels] for labels in train_labels]\n",
    "    train_labels = pad_sequences(train_labels, maxlen=max_length, padding='post', value=num_classes)\n",
    "    train_labels = to_categorical(train_labels, num_classes=num_classes+1)\n",
    "\n",
    "    valid_labels = [[label_to_index[label] for label in labels] for labels in valid_labels]\n",
    "    valid_labels = pad_sequences(valid_labels, maxlen=max_length, padding='post', value=num_classes)\n",
    "    valid_labels = to_categorical(valid_labels, num_classes=num_classes+1)\n",
    "\n",
    "    test_labels = [[label_to_index[label] for label in labels] for labels in test_labels]\n",
    "    test_labels = pad_sequences(test_labels, maxlen=max_length, padding='post', value=num_classes)\n",
    "    test_labels = to_categorical(test_labels, num_classes=num_classes+1)\n",
    "\n",
    "    return (train_sentences, train_labels), (valid_sentences, valid_labels), (test_sentences, test_labels), label_to_index, index_to_label"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "(train_sentences, train_labels), (val_sentences, val_labels), (test_sentences, test_labels), label2id, id2label = load_data(bio_files_dir, MAX_LENGTH)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "(3178, (3178, 200, 35), 454, (454, 200, 35), 909, (909, 200, 35))"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sentences), train_labels.shape, len(val_sentences), val_labels.shape, len(test_sentences), test_labels.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating Sequence and Padding"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Convert the input sentences to sequences of word indices\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "val_sequences = tokenizer.texts_to_sequences(val_sentences)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "train_sequences_padded = pad_sequences(train_sequences, maxlen=MAX_LENGTH, padding='post', truncating='post')\n",
    "val_sequences_padded = pad_sequences(val_sequences, maxlen=MAX_LENGTH, padding='post', truncating='post')\n",
    "test_sequences_padded = pad_sequences(test_sequences, maxlen=MAX_LENGTH, padding='post', truncating='post')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Building Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Define the model architecture\n",
    "model = tf.keras.models.Sequential([\n",
    "    Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_DIM, input_length=MAX_LENGTH),\n",
    "    Bidirectional(LSTM(units=LSTM_UNITS, return_sequences=True)),\n",
    "    Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n",
    "\n",
    "#\n",
    "# model = tf.keras.models.Sequential([\n",
    "#     Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_DIM, input_length=MAX_LENGTH),\n",
    "#     Bidirectional(LSTM(units=LSTM_UNITS, return_sequences=True)),\n",
    "#     Dense(64, activation='relu'),\n",
    "#     Dense(NUM_CLASSES, activation='softmax')\n",
    "# ])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Compile the model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 200, 128)          12800000  \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 200, 128)         98816     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 200, 35)           4515      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,903,331\n",
      "Trainable params: 12,903,331\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-03 18:47:42.570335: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 14s 113ms/step - loss: 0.5558 - accuracy: 0.9535 - val_loss: 0.1623 - val_accuracy: 0.9662\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 11s 108ms/step - loss: 0.1586 - accuracy: 0.9658 - val_loss: 0.1545 - val_accuracy: 0.9662\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 11s 112ms/step - loss: 0.1485 - accuracy: 0.9658 - val_loss: 0.1505 - val_accuracy: 0.9662\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 12s 115ms/step - loss: 0.1351 - accuracy: 0.9659 - val_loss: 0.1485 - val_accuracy: 0.9663\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 11s 110ms/step - loss: 0.1222 - accuracy: 0.9675 - val_loss: 0.1494 - val_accuracy: 0.9669\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 11s 110ms/step - loss: 0.1111 - accuracy: 0.9708 - val_loss: 0.1482 - val_accuracy: 0.9670\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 11s 110ms/step - loss: 0.0982 - accuracy: 0.9739 - val_loss: 0.1476 - val_accuracy: 0.9679\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 11s 111ms/step - loss: 0.0846 - accuracy: 0.9770 - val_loss: 0.1460 - val_accuracy: 0.9682\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 11s 110ms/step - loss: 0.0723 - accuracy: 0.9801 - val_loss: 0.1485 - val_accuracy: 0.9688\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 11s 114ms/step - loss: 0.0622 - accuracy: 0.9826 - val_loss: 0.1490 - val_accuracy: 0.9690\n",
      "29/29 [==============================] - 1s 31ms/step - loss: 0.1578 - accuracy: 0.9681\n",
      "Test accuracy: 0.9680582880973816\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(train_sequences_padded, train_labels, epochs=NUM_EPOCHS, validation_data=(val_sequences_padded, val_labels))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(test_sequences_padded, test_labels)\n",
    "\n",
    "# Print the test accuracy\n",
    "print('Test accuracy:', test_acc)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prediction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "def tokenize_text(text):\n",
    "    # Tokenize the text into a list of words\n",
    "    tokens = []\n",
    "    for sentence in text.split('\\n'):\n",
    "        for word in sentence.split():\n",
    "            # Remove trailing punctuation marks from the word\n",
    "            while word and word[-1] in string.punctuation:\n",
    "                word = word[:-1]\n",
    "            tokens.append(word)\n",
    "    return tokens\n",
    "\n",
    "def predict(text):\n",
    "    # tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "    tokens = [clean_text(token) for token in tokenize_text(text)]\n",
    "\n",
    "    sequence = tokenizer.texts_to_sequences([' '.join(token for token in tokens)])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "    # Make the prediction\n",
    "    prediction = model.predict(np.array(padded_sequence))\n",
    "\n",
    "    # Decode the prediction\n",
    "    predicted_labels = np.argmax(prediction, axis=-1)\n",
    "    predicted_labels = [id2label[i] for i in predicted_labels[0]]\n",
    "\n",
    "    # Print the predicted named entities\n",
    "    print(\"Predicted Named Entities:\")\n",
    "    for i in range(len(tokens)):\n",
    "        print(f\"{tokens[i]}: {predicted_labels[i]}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 472ms/step\n",
      "Predicted Named Entities:\n",
      "the: O\n",
      "patient: O\n",
      "is: O\n",
      "a: O\n",
      "55yearold: B-Disease_disorder\n",
      "male: B-Medication\n",
      "with: O\n",
      "a: O\n",
      "history: O\n",
      "of: O\n",
      "hypertension: I-History\n",
      "and: O\n",
      "diabetes: I-History\n",
      "he: O\n",
      "presented: I-History\n",
      "to: O\n",
      "the: O\n",
      "emergency: O\n",
      "department: O\n",
      "with: O\n",
      "complaints: O\n",
      "of: O\n",
      "chest: B-Biological_structure\n",
      "pain: B-Sign_symptom\n",
      "shortness: B-Sign_symptom\n",
      "of: O\n",
      "breath: I-Sign_symptom\n",
      "and: O\n",
      "dizziness: B-Sign_symptom\n",
      "the: O\n",
      "patients: O\n",
      "blood: B-Diagnostic_procedure\n",
      "pressure: I-Diagnostic_procedure\n",
      "was: O\n",
      "180110: I-Lab_value\n",
      "mmhg: O\n",
      "and: O\n",
      "his: B-Diagnostic_procedure\n",
      "heart: I-Diagnostic_procedure\n",
      "rate: O\n",
      "was: O\n",
      "110: I-Lab_value\n",
      "beats: I-Lab_value\n",
      "per: I-Lab_value\n",
      "minute: O\n"
     ]
    }
   ],
   "source": [
    "predict(\"The patient is a 55-year-old male with a history of hypertension and diabetes. He presented to the emergency department with complaints of chest pain, shortness of breath, and dizziness. The patient's blood pressure was 180/110 mmHg and his heart rate was 110 beats per minute.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 13ms/step\n",
      "Predicted Named Entities:\n",
      "the: O\n",
      "patients: O\n",
      "cranial: B-Biological_structure\n",
      "nerves: I-Biological_structure\n",
      "were: O\n",
      "intact: O\n",
      "during: O\n",
      "the: O\n",
      "physical: B-Diagnostic_procedure\n",
      "exam: O\n"
     ]
    }
   ],
   "source": [
    "predict(\"The patient's cranial nerves were intact during the physical exam.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 12ms/step\n",
      "Predicted Named Entities:\n",
      "the: O\n",
      "patient: O\n",
      "presented: O\n",
      "with: O\n",
      "acute: O\n",
      "abdominal: B-Biological_structure\n",
      "pain: B-Sign_symptom\n",
      "nausea: B-Sign_symptom\n",
      "and: O\n",
      "vomiting: B-Sign_symptom\n",
      "and: O\n",
      "was: O\n",
      "diagnosed: O\n",
      "with: O\n",
      "acute: O\n",
      "appendicitis: B-Disease_disorder\n"
     ]
    }
   ],
   "source": [
    "predict(\"The patient presented with acute abdominal pain, nausea, and vomiting, and was diagnosed with acute appendicitis.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 12ms/step\n",
      "Predicted Named Entities:\n",
      "the: O\n",
      "biopsies: B-Diagnostic_procedure\n",
      "revealed: O\n",
      "the: O\n",
      "presence: O\n",
      "of: O\n",
      "malignancy: B-Sign_symptom\n",
      "in: O\n",
      "the: O\n",
      "patients: O\n",
      "tissue: O\n",
      "samples: O\n"
     ]
    }
   ],
   "source": [
    "predict(\"The biopsies revealed the presence of malignancy in the patient's tissue samples.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 13ms/step\n",
      "Predicted Named Entities:\n",
      "the: O\n",
      "patient: O\n",
      "was: O\n",
      "prescribed: O\n",
      "prednisone: B-Medication\n",
      "to: O\n",
      "help: O\n",
      "manage: O\n",
      "their: B-Medication\n",
      "autoimmune: B-Medication\n",
      "disorder: O\n"
     ]
    }
   ],
   "source": [
    "predict(\"The patient was prescribed prednisone to help manage their autoimmune disorder.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 10ms/step\n",
      "Predicted Named Entities:\n",
      "the: O\n",
      "patient: O\n",
      "underwent: O\n",
      "successful: O\n",
      "removal: O\n",
      "of: O\n",
      "a: O\n",
      "nodule: B-Sign_symptom\n",
      "from: O\n",
      "their: O\n",
      "thyroid: B-Disease_disorder\n",
      "gland: O\n"
     ]
    }
   ],
   "source": [
    "predict(\"The patient underwent successful removal of a nodule from their thyroid gland.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "{1: 'B-Age',\n 2: 'B-Biological_attribute',\n 3: 'B-Biological_structure',\n 4: 'B-Clinical_event',\n 5: 'B-Diagnostic_procedure',\n 6: 'B-Disease_disorder',\n 7: 'B-Dosage',\n 8: 'B-Family_history',\n 9: 'B-Height',\n 10: 'B-History',\n 11: 'B-Lab_value',\n 12: 'B-Mass',\n 13: 'B-Medication',\n 14: 'B-Sex',\n 15: 'B-Sign_symptom',\n 16: 'B-Therapeutic_procedure',\n 17: 'B-Weight',\n 18: 'I-Age',\n 19: 'I-Biological_attribute',\n 20: 'I-Biological_structure',\n 21: 'I-Clinical_event',\n 22: 'I-Diagnostic_procedure',\n 23: 'I-Disease_disorder',\n 24: 'I-Dosage',\n 25: 'I-Family_history',\n 26: 'I-Height',\n 27: 'I-History',\n 28: 'I-Lab_value',\n 29: 'I-Mass',\n 30: 'I-Medication',\n 31: 'I-Sign_symptom',\n 32: 'I-Therapeutic_procedure',\n 33: 'I-Weight',\n 34: 'O',\n 0: '<PAD>'}"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
