{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_files_dir = '../data/NEW_BIO_FILES'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 500000\n",
    "EMBEDDING_DIM = 512\n",
    "MAX_LENGTH = 200\n",
    "LSTM_UNITS = 64\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "TEST_SIZE = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Remove punctuation and convert to lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text).lower()\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data(data_dir, max_length=None):\n",
    "\n",
    "    # Load all files in the data directory\n",
    "    all_files = os.listdir(data_dir)\n",
    "\n",
    "    # Filter only the files with the .bio extension\n",
    "    bio_files = [f for f in all_files if f.endswith('.bio')]\n",
    "\n",
    "    # Initialize lists to hold sentences and labels\n",
    "    sentences = []\n",
    "    labels = []\n",
    "\n",
    "    # Loop through each file and read the sentences and labels\n",
    "    for file in bio_files:\n",
    "        with open(os.path.join(data_dir, file), 'r', encoding='utf-8') as f:\n",
    "            current_sentences = []\n",
    "            current_labels = []\n",
    "            for line in f:\n",
    "                if line.strip() == '':\n",
    "                    # If we encounter a blank line, it means we've reached the end of a sentence\n",
    "                    if len(current_sentences) > 0:\n",
    "                        # Add the current sentence and labels to the list\n",
    "                        sentences.append(current_sentences)\n",
    "                        labels.append(current_labels)\n",
    "                        # Reset the current sentence and labels lists\n",
    "                        current_sentences = []\n",
    "                        current_labels = []\n",
    "                else:\n",
    "                    # Otherwise, split the line into its word and label components\n",
    "                    word, label = line.strip().split('\\t')\n",
    "                    current_sentences.append(clean_text(word))\n",
    "                    current_labels.append(label)\n",
    "\n",
    "    # Shuffle the sentences and labels\n",
    "    combined = list(zip(sentences, labels))\n",
    "    random.shuffle(combined)\n",
    "    sentences[:], labels[:] = zip(*combined)\n",
    "\n",
    "    # Split the data into training, validation, and test sets\n",
    "\n",
    "    num_sentences = len(sentences)\n",
    "    num_train = int(num_sentences * (1 - TEST_SIZE - 0.1))\n",
    "    num_valid = int(num_sentences * 0.1)\n",
    "\n",
    "    train_sentences = sentences[:num_train]\n",
    "    train_labels = labels[:num_train]\n",
    "\n",
    "    valid_sentences = sentences[num_train:num_train+num_valid]\n",
    "    valid_labels = labels[num_train:num_train+num_valid]\n",
    "\n",
    "    test_sentences = sentences[num_train+num_valid:]\n",
    "    test_labels = labels[num_train+num_valid:]\n",
    "\n",
    "    # Convert the labels to one-hot encoding\n",
    "    unique_labels = set(element for sublist in labels for element in sublist)\n",
    "    label_to_index = {label: id+1 for id, label in enumerate(sorted(unique_labels))}\n",
    "    index_to_label = {id: label for label, id in label_to_index.items()}\n",
    "\n",
    "    # Add the new label and ID to the dictionaries\n",
    "    label_to_index['<PAD>'] = 0\n",
    "    index_to_label[0] = '<PAD>'\n",
    "\n",
    "    num_classes = len(index_to_label) - 1\n",
    "\n",
    "    train_labels = [[label_to_index[label] for label in labels] for labels in train_labels]\n",
    "    train_labels = pad_sequences(train_labels, maxlen=max_length, padding='post', value=num_classes)\n",
    "    train_labels = to_categorical(train_labels, num_classes=num_classes+1)\n",
    "\n",
    "    valid_labels = [[label_to_index[label] for label in labels] for labels in valid_labels]\n",
    "    valid_labels = pad_sequences(valid_labels, maxlen=max_length, padding='post', value=num_classes)\n",
    "    valid_labels = to_categorical(valid_labels, num_classes=num_classes+1)\n",
    "\n",
    "    test_labels = [[label_to_index[label] for label in labels] for labels in test_labels]\n",
    "    test_labels = pad_sequences(test_labels, maxlen=max_length, padding='post', value=num_classes)\n",
    "    test_labels = to_categorical(test_labels, num_classes=num_classes+1)\n",
    "\n",
    "    return (train_sentences, train_labels), (valid_sentences, valid_labels), (test_sentences, test_labels), label_to_index, index_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_sentences, train_labels), (val_sentences, val_labels), (test_sentences, test_labels), label2id, id2label = load_data(bio_files_dir, MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = len(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3178, (3178, 200, 35), 454, (454, 200, 35), 909, (909, 200, 35))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sentences), train_labels.shape, len(val_sentences), val_labels.shape, len(test_sentences), test_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Sequence and Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the input sentences to sequences of word indices\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "val_sequences = tokenizer.texts_to_sequences(val_sentences)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "train_sequences_padded = pad_sequences(train_sequences, maxlen=MAX_LENGTH, padding='post', truncating='post')\n",
    "val_sequences_padded = pad_sequences(val_sequences, maxlen=MAX_LENGTH, padding='post', truncating='post')\n",
    "test_sequences_padded = pad_sequences(test_sequences, maxlen=MAX_LENGTH, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture\n",
    "model = tf.keras.models.Sequential([\n",
    "    Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_DIM, input_length=MAX_LENGTH),\n",
    "    Bidirectional(LSTM(units=LSTM_UNITS, return_sequences=True)),\n",
    "    Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n",
    "\n",
    "#\n",
    "# model = tf.keras.models.Sequential([\n",
    "#     Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_DIM, input_length=MAX_LENGTH),\n",
    "#     Bidirectional(LSTM(units=LSTM_UNITS, return_sequences=True)),\n",
    "#     Dense(64, activation='relu'),\n",
    "#     Dense(NUM_CLASSES, activation='softmax')\n",
    "# ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 200, 512)          256000512 \n",
      "                                                                 \n",
      " bidirectional_10 (Bidirecti  (None, 200, 128)         295424    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 200, 35)           4515      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 256,300,451\n",
      "Trainable params: 256,300,451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 15/100 [===>..........................] - ETA: 53s - loss: 1.6075 - accuracy: 0.8995"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_sequences_padded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval_sequences_padded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m test_loss, test_acc \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(test_sequences_padded, test_labels)\n",
      "File \u001b[0;32m~/dev/projects/NER-medical-text/lib/python3.8/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/dev/projects/NER-medical-text/lib/python3.8/site-packages/keras/engine/training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1403\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1404\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   1405\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[1;32m   1406\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1407\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1408\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1409\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1410\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1411\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/dev/projects/NER-medical-text/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/dev/projects/NER-medical-text/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/dev/projects/NER-medical-text/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/dev/projects/NER-medical-text/lib/python3.8/site-packages/tensorflow/python/eager/function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2450\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2451\u001b[0m   (graph_function,\n\u001b[1;32m   2452\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/projects/NER-medical-text/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1856\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1859\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1860\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1862\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m     args,\n\u001b[1;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1865\u001b[0m     executing_eagerly)\n\u001b[1;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/dev/projects/NER-medical-text/lib/python3.8/site-packages/tensorflow/python/eager/function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    496\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 497\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    503\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    505\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    506\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    509\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    510\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/dev/projects/NER-medical-text/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(train_sequences_padded, train_labels, epochs=NUM_EPOCHS, validation_data=(val_sequences_padded, val_labels))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(test_sequences_padded, test_labels)\n",
    "\n",
    "# Print the test accuracy\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "def tokenize_text(text):\n",
    "    # Tokenize the text into a list of words\n",
    "    tokens = []\n",
    "    for sentence in text.split('\\n'):\n",
    "        for word in sentence.split():\n",
    "            # Remove trailing punctuation marks from the word\n",
    "            while word and word[-1] in string.punctuation:\n",
    "                word = word[:-1]\n",
    "            tokens.append(word)\n",
    "    return tokens\n",
    "\n",
    "def predict(text):\n",
    "    # tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "    tokens = [clean_text(token) for token in tokenize_text(text)]\n",
    "\n",
    "    sequence = tokenizer.texts_to_sequences([' '.join(token for token in tokens)])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "    # Make the prediction\n",
    "    prediction = model.predict(np.array(padded_sequence))\n",
    "\n",
    "    # Decode the prediction\n",
    "    predicted_labels = np.argmax(prediction, axis=-1)\n",
    "    predicted_labels = [id2label[i] for i in predicted_labels[0]]\n",
    "\n",
    "    # Print the predicted named entities\n",
    "    print(\"Predicted Named Entities:\")\n",
    "    for i in range(len(tokens)):\n",
    "        print(f\"{tokens[i]}: {predicted_labels[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 434ms/step\n",
      "Predicted Named Entities:\n",
      "the: O\n",
      "patient: O\n",
      "is: O\n",
      "a: O\n",
      "55yearold: O\n",
      "male: O\n",
      "with: O\n",
      "a: O\n",
      "history: O\n",
      "of: O\n",
      "hypertension: O\n",
      "and: O\n",
      "diabetes: O\n",
      "he: O\n",
      "presented: O\n",
      "to: O\n",
      "the: O\n",
      "emergency: O\n",
      "department: O\n",
      "with: O\n",
      "complaints: O\n",
      "of: O\n",
      "chest: O\n",
      "pain: O\n",
      "shortness: O\n",
      "of: O\n",
      "breath: O\n",
      "and: O\n",
      "dizziness: O\n",
      "the: O\n",
      "patients: O\n",
      "blood: O\n",
      "pressure: O\n",
      "was: O\n",
      "180110: O\n",
      "mmhg: O\n",
      "and: O\n",
      "his: O\n",
      "heart: O\n",
      "rate: O\n",
      "was: O\n",
      "110: O\n",
      "beats: O\n",
      "per: O\n",
      "minute: O\n"
     ]
    }
   ],
   "source": [
    "predict(\"The patient is a 55-year-old male with a history of hypertension and diabetes. He presented to the emergency department with complaints of chest pain, shortness of breath, and dizziness. The patient's blood pressure was 180/110 mmHg and his heart rate was 110 beats per minute.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 13ms/step\n",
      "Predicted Named Entities:\n",
      "the: O\n",
      "patients: O\n",
      "cranial: O\n",
      "nerves: O\n",
      "were: O\n",
      "intact: O\n",
      "during: O\n",
      "the: O\n",
      "physical: O\n",
      "exam: O\n"
     ]
    }
   ],
   "source": [
    "predict(\"The patient's cranial nerves were intact during the physical exam.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 13ms/step\n",
      "Predicted Named Entities:\n",
      "the: O\n",
      "patient: O\n",
      "presented: O\n",
      "with: O\n",
      "acute: O\n",
      "abdominal: O\n",
      "pain: O\n",
      "nausea: O\n",
      "and: O\n",
      "vomiting: O\n",
      "and: O\n",
      "was: O\n",
      "diagnosed: O\n",
      "with: O\n",
      "acute: O\n",
      "appendicitis: O\n"
     ]
    }
   ],
   "source": [
    "predict(\"The patient presented with acute abdominal pain, nausea, and vomiting, and was diagnosed with acute appendicitis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 13ms/step\n",
      "Predicted Named Entities:\n",
      "the: O\n",
      "biopsies: O\n",
      "revealed: O\n",
      "the: O\n",
      "presence: O\n",
      "of: O\n",
      "malignancy: O\n",
      "in: O\n",
      "the: O\n",
      "patients: O\n",
      "tissue: O\n",
      "samples: O\n"
     ]
    }
   ],
   "source": [
    "predict(\"The biopsies revealed the presence of malignancy in the patient's tissue samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 13ms/step\n",
      "Predicted Named Entities:\n",
      "the: O\n",
      "patient: O\n",
      "was: O\n",
      "prescribed: O\n",
      "prednisone: O\n",
      "to: O\n",
      "help: O\n",
      "manage: O\n",
      "their: O\n",
      "autoimmune: O\n",
      "disorder: O\n"
     ]
    }
   ],
   "source": [
    "predict(\"The patient was prescribed prednisone to help manage their autoimmune disorder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 10ms/step\n",
      "Predicted Named Entities:\n",
      "the: O\n",
      "patient: O\n",
      "underwent: O\n",
      "successful: O\n",
      "removal: O\n",
      "of: O\n",
      "a: O\n",
      "nodule: O\n",
      "from: O\n",
      "their: O\n",
      "thyroid: O\n",
      "gland: O\n"
     ]
    }
   ],
   "source": [
    "predict(\"The patient underwent successful removal of a nodule from their thyroid gland.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'B-MedicalCondition',\n",
       " 2: 'B-Medicine',\n",
       " 3: 'B-Pathogen',\n",
       " 4: 'I-MedicalCondition',\n",
       " 5: 'I-Medicine',\n",
       " 6: 'I-Pathogen',\n",
       " 7: 'O',\n",
       " 0: '<PAD>'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The class with index 7 has the maximum number of examples: 15076.0\n"
     ]
    }
   ],
   "source": [
    "num_examples, max_seq_length, num_classes = train_labels.shape\n",
    "\n",
    "class_counts = np.sum(train_labels, axis=(0, 1))  # sum along first two axes to get class counts\n",
    "\n",
    "max_class_index = np.argmax(class_counts)\n",
    "\n",
    "print(f\"The class with index {max_class_index} has the maximum number of examples: {class_counts[max_class_index]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'O'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num_class with maximum examples: 34\n",
      "Number of examples: 613156.0\n",
      "Indices in sorting order of which has maximum examples:\n",
      "Num_class 34 : 613156.0\n",
      "Num_class 5 : 3231.0\n",
      "Num_class 22 : 2440.0\n",
      "Num_class 15 : 2328.0\n",
      "Num_class 3 : 2041.0\n",
      "Num_class 11 : 1922.0\n",
      "Num_class 20 : 1654.0\n",
      "Num_class 28 : 1459.0\n",
      "Num_class 31 : 981.0\n",
      "Num_class 6 : 905.0\n",
      "Num_class 27 : 892.0\n",
      "Num_class 13 : 752.0\n",
      "Num_class 16 : 667.0\n",
      "Num_class 23 : 555.0\n",
      "Num_class 4 : 425.0\n",
      "Num_class 24 : 390.0\n",
      "Num_class 32 : 383.0\n",
      "Num_class 25 : 313.0\n",
      "Num_class 10 : 245.0\n",
      "Num_class 7 : 238.0\n",
      "Num_class 30 : 194.0\n",
      "Num_class 1 : 140.0\n",
      "Num_class 14 : 132.0\n",
      "Num_class 8 : 67.0\n",
      "Num_class 21 : 40.0\n",
      "Num_class 18 : 29.0\n",
      "Num_class 2 : 6.0\n",
      "Num_class 17 : 3.0\n",
      "Num_class 33 : 3.0\n",
      "Num_class 29 : 2.0\n",
      "Num_class 12 : 2.0\n",
      "Num_class 9 : 2.0\n",
      "Num_class 26 : 2.0\n",
      "Num_class 19 : 1.0\n",
      "Num_class 0 : 0.0\n"
     ]
    }
   ],
   "source": [
    "# get the shape of the array\n",
    "shape = train_labels.shape\n",
    "\n",
    "# loop over the first axis and count the number of examples\n",
    "count = np.zeros(shape[2])\n",
    "for i in range(shape[0]):\n",
    "    for j in range(shape[1]):\n",
    "        for k in range(shape[2]):\n",
    "            if train_labels[i, j, k] == 1:\n",
    "                count[k] += 1\n",
    "\n",
    "# find the num_class with the maximum number of examples\n",
    "max_class = np.argmax(count)\n",
    "\n",
    "print(\"Num_class with maximum examples:\", max_class)\n",
    "print(\"Number of examples:\", count[max_class])\n",
    "\n",
    "# print the indices in sorting order of which has maximum examples\n",
    "print(\"Indices in sorting order of which has maximum examples:\")\n",
    "for i in np.argsort(-count):\n",
    "    print(\"Num_class\", i, \":\", count[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'B-Diagnostic_procedure'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CRF model\n",
    "def crf_model(features, labels, mode):\n",
    "    # Create a word embeddings layer\n",
    "    word_embeddings = tf.keras.layers.Embedding(input_dim=VOCAB_SIZE, output_dim=50)(features['word'])\n",
    "\n",
    "    # Create a Bidirectional LSTM layer\n",
    "    lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=50, return_sequences=True))(word_embeddings)\n",
    "\n",
    "    # Create a CRF layer\n",
    "    crf = tf.keras.layers.CRF(VOCAB_SIZE, name='crf_layer')\n",
    "    output = crf(lstm)\n",
    "\n",
    "    # Compile the model\n",
    "    model = tf.keras.Model(inputs=features, outputs=output)\n",
    "    model.compile(optimizer='adam', loss=crf.loss, metrics=[crf.accuracy])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3178, 200), (3178, 200, 35))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sequences_padded.shape, train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "80/80 [==============================] - 8s 61ms/step - loss: 0.1852 - accuracy: 0.5713 - val_loss: 0.1498 - val_accuracy: 0.6102\n",
      "Epoch 2/10\n",
      "80/80 [==============================] - 4s 45ms/step - loss: 0.1491 - accuracy: 0.6138 - val_loss: 0.1458 - val_accuracy: 0.6102\n",
      "Epoch 3/10\n",
      "80/80 [==============================] - 4s 45ms/step - loss: 0.1461 - accuracy: 0.6138 - val_loss: 0.1440 - val_accuracy: 0.6102\n",
      "Epoch 4/10\n",
      "80/80 [==============================] - 4s 45ms/step - loss: 0.1443 - accuracy: 0.6138 - val_loss: 0.1413 - val_accuracy: 0.6102\n",
      "Epoch 5/10\n",
      "80/80 [==============================] - 4s 45ms/step - loss: 0.1422 - accuracy: 0.6138 - val_loss: 0.1399 - val_accuracy: 0.6104\n",
      "Epoch 6/10\n",
      "80/80 [==============================] - 4s 46ms/step - loss: 0.1407 - accuracy: 0.6144 - val_loss: 0.1383 - val_accuracy: 0.6115\n",
      "Epoch 7/10\n",
      "80/80 [==============================] - 4s 46ms/step - loss: 0.1392 - accuracy: 0.6153 - val_loss: 0.1369 - val_accuracy: 0.6120\n",
      "Epoch 8/10\n",
      "80/80 [==============================] - 4s 46ms/step - loss: 0.1379 - accuracy: 0.6158 - val_loss: 0.1360 - val_accuracy: 0.6125\n",
      "Epoch 9/10\n",
      "80/80 [==============================] - 4s 46ms/step - loss: 0.1368 - accuracy: 0.6157 - val_loss: 0.1348 - val_accuracy: 0.6123\n",
      "Epoch 10/10\n",
      "80/80 [==============================] - 4s 46ms/step - loss: 0.1360 - accuracy: 0.6159 - val_loss: 0.1338 - val_accuracy: 0.6123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x296d37310>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "# Define the CRF model\n",
    "def crf_model(input_shape, num_labels):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Masking(mask_value=0, input_shape=input_shape),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=50, return_sequences=True)),\n",
    "        tf.keras.layers.Dense(num_labels),\n",
    "        tf.keras.layers.Activation('softmax')\n",
    "    ])\n",
    "\n",
    "    # crf = sklearn_crfsuite.metrics.flat_f1_score\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Remove the extra dimension and add the timesteps dimension to train_sequences_padded\n",
    "# train_sequences_padded = np.expand_dims(train_sequences_padded, axis=-1)\n",
    "\n",
    "# Define the input shape and number of labels\n",
    "input_shape = train_sequences_padded.shape[1:]\n",
    "num_labels = train_labels.shape[-1]\n",
    "\n",
    "# Define the CRF model\n",
    "model = crf_model(input_shape, num_labels)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_sequences_padded, train_labels, batch_size=32, epochs=10, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3178, 200)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sequences_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3178, 200), (3178, 200, 35))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sequences_padded.shape, train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    # Tokenize the text into a list of words\n",
    "    tokens = []\n",
    "    for sentence in re.split(r'\\n', text):\n",
    "        for word in sentence.split():\n",
    "\n",
    "            # Remove the format [%d]\n",
    "            word = re.sub(r'\\[?\\d+\\]', '', word)\n",
    "\n",
    "            word = word.strip()\n",
    "\n",
    "            # Remove trailing punctuation marks from the word\n",
    "            while word and word[-1] in string.punctuation:\n",
    "                word = word[:-1]\n",
    "\n",
    "            # Remove leading punctutation marks from the word\n",
    "            while word and word[0] in string.punctuation:\n",
    "                word = word[1:]\n",
    "            if word:\n",
    "                tokens.append(word)\n",
    "\n",
    "        if tokens[-1] != \"<NEWL>\":\n",
    "            tokens.append(\"<NEWL>\")\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(text):\n",
    "    # tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "    tokens = [clean_text(token) for token in tokenize_text(text)]\n",
    "\n",
    "    sequence = tokenizer.texts_to_sequences([' '.join(token for token in tokens)])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "    # Add an extra dimension to match the input shape of the model\n",
    "    padded_sequence_with_batch_size = np.expand_dims(padded_sequence, axis=-1)\n",
    "\n",
    "    # Make the prediction\n",
    "    prediction = model.predict(padded_sequence_with_batch_size)\n",
    "\n",
    "    # Decode the prediction\n",
    "    predicted_labels = np.argmax(prediction, axis=-1)\n",
    "    predicted_labels = [id2label[i] for i in predicted_labels[0]]\n",
    "\n",
    "    # Print the predicted named entities\n",
    "    print(\"Predicted Named Entities:\")\n",
    "    for i in range(len(tokens)):\n",
    "        print(f\"{tokens[i]}: {predicted_labels[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "Predicted Named Entities:\n",
      "a: O\n",
      "54yearold: O\n",
      "man: O\n",
      "with: O\n",
      "a: O\n",
      "history: O\n",
      "of: O\n",
      "hypertension: O\n",
      "hyperlipidemia: O\n",
      "and: O\n",
      "a: O\n",
      "previous: O\n",
      "myocardial: O\n",
      "infarction: O\n",
      "presents: O\n",
      "to: O\n",
      "the: O\n",
      "emergency: O\n",
      "department: O\n",
      "with: O\n",
      "severe: O\n",
      "chest: O\n",
      "pain: O\n",
      "he: O\n",
      "reports: O\n",
      "the: O\n",
      "pain: O\n",
      "began: O\n",
      "suddenly: O\n",
      "and: O\n",
      "has: O\n",
      "been: O\n",
      "getting: O\n",
      "progressively: O\n",
      "worse: O\n",
      "over: O\n",
      "the: O\n",
      "last: O\n",
      "hour: O\n",
      "he: O\n",
      "also: O\n",
      "reports: O\n",
      "shortness: O\n",
      "of: O\n",
      "breath: O\n",
      "and: O\n",
      "nausea: O\n",
      "on: O\n",
      "physical: O\n",
      "exam: O\n",
      "his: O\n",
      "blood: O\n",
      "pressure: O\n",
      "is: O\n",
      "180100: O\n",
      "mmhg: O\n",
      "heart: O\n",
      "rate: O\n",
      "is: O\n",
      "120: O\n",
      "beats: O\n",
      "per: O\n",
      "minute: O\n",
      "and: O\n",
      "respiratory: O\n",
      "rate: O\n",
      "is: O\n",
      "24: O\n",
      "breaths: O\n",
      "per: O\n",
      "minute: O\n",
      "an: O\n",
      "electrocardiogram: O\n",
      "reveals: O\n",
      "stsegment: O\n",
      "elevation: O\n",
      "in: O\n",
      "leads: O\n",
      "ii: O\n",
      "iii: O\n",
      "and: O\n",
      "avf: O\n",
      "the: O\n",
      "patient: O\n",
      "is: O\n",
      "immediately: O\n",
      "started: O\n",
      "on: O\n",
      "aspirin: O\n",
      "heparin: O\n",
      "and: O\n",
      "nitroglycerin: O\n",
      "and: O\n",
      "is: O\n",
      "taken: O\n",
      "to: O\n",
      "the: O\n",
      "cardiac: O\n",
      "catheterization: O\n",
      "lab: O\n",
      "for: O\n",
      "emergent: O\n",
      "angiography: O\n",
      "and: O\n",
      "possible: O\n",
      "percutaneous: B-Disease_disorder\n",
      "coronary: B-Disease_disorder\n",
      "intervention: B-Disease_disorder\n",
      "newl: B-Disease_disorder\n"
     ]
    }
   ],
   "source": [
    "predict(\"A 54-year-old man with a history of hypertension, hyperlipidemia, and a previous myocardial infarction presents to the emergency department with severe chest pain. He reports the pain began suddenly and has been getting progressively worse over the last hour. He also reports shortness of breath and nausea. On physical exam, his blood pressure is 180/100 mmHg, heart rate is 120 beats per minute, and respiratory rate is 24 breaths per minute. An electrocardiogram reveals ST-segment elevation in leads II, III, and aVF. The patient is immediately started on aspirin, heparin, and nitroglycerin, and is taken to the cardiac catheterization lab for emergent angiography and possible percutaneous coronary intervention.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume you have a new input sequence stored in a variable called 'new_sequence'\n",
    "# It should be of shape (1, sequence_length), where sequence_length is the length of the sequence you want to label\n",
    "\n",
    "# Pad the new sequence to make it the same length as the training sequences\n",
    "new_sequence_padded = pad_sequences([new_sequence], maxlen=MAX_LENGTH, padding='post', truncating='post')\n",
    "\n",
    "# Add an extra dimension to match the input shape of the model\n",
    "new_sequence_padded = np.expand_dims(new_sequence_padded, axis=-1)\n",
    "\n",
    "# Use the model to predict the labels for the new sequence\n",
    "predicted_labels = model.predict(new_sequence_padded)\n",
    "\n",
    "# The predicted_labels variable will be an array of shape (1, sequence_length, num_labels)\n",
    "# You can use np.argmax to get the index of the highest probability label for each token in the sequence\n",
    "predicted_labels = np.argmax(predicted_labels, axis=-1)\n",
    "\n",
    "# The predicted_labels variable will now be an array of shape (1, sequence_length) containing the predicted labels for each token in the sequence\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
