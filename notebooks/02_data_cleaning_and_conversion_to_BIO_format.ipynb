{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a43dc53f",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6d9a8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1131)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8340a648",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data/annotated_json_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1dc98c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_to_acronyms = {\n",
    "    'Activity': 'ACT',\n",
    "    'Administration': 'ADM',\n",
    "    'Age': 'AGE',\n",
    "    'Area': 'ARA',\n",
    "    'Biological_attribute': 'BAT',\n",
    "    'Biological_structure': 'BST',\n",
    "    'Clinical_event': 'CLE',\n",
    "    'Color': 'COL',\n",
    "    'Coreference': 'COR',\n",
    "    'Date': 'DAT',\n",
    "    'Detailed_description': 'DET',\n",
    "    'Diagnostic_procedure': 'DIA',\n",
    "    'Disease_disorder': 'DIS',\n",
    "    'Distance': 'DIS',\n",
    "    'Dosage': 'DOS',\n",
    "    'Duration': 'DUR',\n",
    "    'Family_history': 'FAM',\n",
    "    'Frequency': 'FRE',\n",
    "    'Height': 'HEI',\n",
    "    'History': 'HIS',\n",
    "    'Lab_value': 'LAB',\n",
    "    'Mass': 'MAS',\n",
    "    'Medication': 'MED',\n",
    "    'Nonbiological_location': 'NBL',\n",
    "    'Occupation': 'OCC',\n",
    "    'Other_entity': 'OTH',\n",
    "    'Other_event': 'OTE',\n",
    "    'Outcome': 'OUT',\n",
    "    'Personal_background': 'PER',\n",
    "    'Qualitative_concept': 'QUC',\n",
    "    'Quantitative_concept': 'QUC',\n",
    "    'Severity': 'SEV',\n",
    "    'Sex': 'SEX',\n",
    "    'Shape': 'SHA',\n",
    "    'Sign_symptom': 'SIG',\n",
    "    'Subject': 'SUB',\n",
    "    'Texture': 'TEX',\n",
    "    'Therapeutic_procedure': 'THP',\n",
    "    'Time': 'TIM',\n",
    "    'Volume': 'VOL',\n",
    "    'Weight': 'WEI'\n",
    "}\n",
    "\n",
    "\n",
    "acronyms_to_entities = {v: k for k, v in entity_to_acronyms.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5ecee9",
   "metadata": {},
   "source": [
    "## Step 1 - Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62d56618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the JSON file for reading\n",
    "with open(os.path.join(data_dir, \"annotated_data.json\"), 'r') as f:\n",
    "\n",
    "    # Load the JSON data into a dictionary\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37405cd",
   "metadata": {},
   "source": [
    "## Step 2 - Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "36a4b056",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_trailing_punctuation(token):\n",
    "    \"\"\"\n",
    "    Removes trailing punctuation from a token.\n",
    "\n",
    "    Args:\n",
    "        token (str): A string representing the token to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned token with trailing punctuation removed.\n",
    "    \"\"\"\n",
    "    while token and re.search(r'[^\\w\\s\\']', token[-1]):\n",
    "        token = token[:-1]\n",
    "        \n",
    "    return token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c14c8a",
   "metadata": {},
   "source": [
    "`split_text` function that takes in a text as input and returns three lists:\n",
    "\n",
    "* tokens: a list of words (with trailing punctuation removed)\n",
    "* start_end_ranges: a list of tuples representing the start and end indices of each word in the original text\n",
    "* sentence_breaks: a list of indices indicating the positions in the tokens list where a new sentence begins.\n",
    "\n",
    "The function first defines a regular expression pattern to match non-space and non-dash characters. It then initializes empty lists for tokens, start_end_ranges, and sentence_breaks.\n",
    "\n",
    "\n",
    "The function then iterates over each sentence in the input text, finds the words in each sentence using regex matching, removes trailing punctuation from each word using another function, and calculates the start and end indices for each word.\n",
    "\n",
    "\n",
    "The function updates the start and end indices to account for the sentence's position in the entire text, adds the indices and words to the respective lists, and appends the index of the last word in the sentence to the sentence_breaks list.\n",
    "\n",
    "\n",
    "Finally, the function returns the three lists containing the processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6febeb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text):\n",
    "\n",
    "    regex_match = r'[^\\s\\u200a\\-\\u2010-\\u2015\\u2212\\uff0d]+'  # r'[^\\s\\u200a\\-\\—\\–]+'\n",
    "\n",
    "    tokens = []\n",
    "    start_end_ranges = []\n",
    "\n",
    "    sentence_breaks = []\n",
    "\n",
    "    start_idx = 0\n",
    "\n",
    "    for sentence in text.split('\\n'):\n",
    "        words = [match.group(0) for match in re.finditer(regex_match, sentence)]\n",
    "        processed_words = list(map(remove_trailing_punctuation, words))\n",
    "        sentence_indices = [(match.start(), match.start() + len(token)) for match, token in\n",
    "                            zip(re.finditer(regex_match, sentence), processed_words)]\n",
    "\n",
    "        # Update the indices to account for the current sentence's position in the entire text\n",
    "        sentence_indices = [(start_idx + start, start_idx + end) for start, end in sentence_indices]\n",
    "\n",
    "        start_end_ranges.extend(sentence_indices)\n",
    "        tokens.extend(processed_words)\n",
    "\n",
    "        sentence_breaks.append(len(tokens))\n",
    "\n",
    "        start_idx += len(sentence) + 1\n",
    "    return tokens, start_end_ranges, sentence_breaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8f24ce89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['Our', '24', 'year', 'old', 'non', 'smoking', 'male', 'patient', 'presented', 'with', 'repeated', 'hemoptysis', 'in', 'May', '2008', 'with', '4', 'days'], [(0, 3), (4, 6), (7, 11), (12, 15), (16, 19), (20, 27), (28, 32), (33, 40), (41, 50), (51, 55), (56, 64), (65, 75), (76, 78), (79, 82), (83, 87), (88, 92), (93, 94), (95, 99)], [18])\n"
     ]
    }
   ],
   "source": [
    "for doc_id, doc in data.items():\n",
    "    print(split_text(doc['text'][:100]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decc4513",
   "metadata": {},
   "source": [
    "## Step 3 - Convert to BIO format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdefced",
   "metadata": {},
   "source": [
    "`tag_token` function adds a tag label to a token at a given position in a sequence of tags, based on the position of the previous token and whether the current token has the same tag label as the previous token. It takes in a list of tag labels, a position in the list, and the tag label to add. If the current token is not the first in the sequence and the previous token has the same tag label as the current token, then the tag label is added as an \"I-\" tag. Otherwise, the tag label is added as a \"B-\" tag. The function modifies the original list and does not return any value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bde515db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_token(tokens, tags, token_pos, entity):\n",
    "    \"\"\"\n",
    "    Modifies a list of tags by adding a tag label to a token at a given position in the list, based on the position of the \n",
    "    previous token and whether the current token has the same tag label as the previous token.\n",
    "\n",
    "    Args:\n",
    "    - tokens (list): A list of tokens in a sequence.\n",
    "    - tags (list): A list of tag labels corresponding to the tokens in a sequence.\n",
    "    - token_pos (int): The position of the token to tag.\n",
    "    - entity (str): The tag label to add to the token.\n",
    "\n",
    "    Returns:\n",
    "    - tags (list): The modified list of tag labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    tag = entity_to_acronyms[entity]\n",
    "    \n",
    "    if token_pos > 0 and f'{tag}' in tags[token_pos - 1]:        \n",
    "            tags[token_pos] = f'I-{tag}'\n",
    "    elif tokens[token_pos] not in stop_words:\n",
    "            tags[token_pos] = f'B-{tag}'\n",
    "            \n",
    "    return tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d4a93f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_bio_files(output_file_path, tokens, tags, sentence_breaks):\n",
    "\n",
    "    # Write the tags to a .bio file\n",
    "    with open(output_file_path, 'w') as f:\n",
    "        for i in range(len(tokens)):\n",
    "            token = tokens[i].strip()\n",
    "            if token:\n",
    "                if i in sentence_breaks:\n",
    "                    f.write(\"\\n\")\n",
    "                f.write(f\"{tokens[i]}\\t{tags[i]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "91c0fcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_ann_to_bio(data, output_dir, filtered_entities=[]):\n",
    "    \n",
    "    \"\"\"\n",
    "    Convert annotations from a dictionary of text files to a BIO-tagged sequence.\n",
    "\n",
    "    Args:\n",
    "        data (dict): A dictionary of text files where keys are file IDs and values are dictionaries containing 'text' and\n",
    "            'annotations' keys.\n",
    "        filtered_entities (list): A list of entity labels to include. If provided, only annotations with these labels will\n",
    "            be converted to the BIO format. Defaults to an empty list.\n",
    "\n",
    "    Returns:\n",
    "        A tuple of two lists: tokens and tags.\n",
    "        - tokens (list): A list of tokens in a sequence.\n",
    "        - tags (list): A list of corresponding tags for each token in the sequence. Tags are BIO formatted.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    if os.path.exists(output_dir):\n",
    "        # Delete the contents of the directory\n",
    "        shutil.rmtree(output_dir)\n",
    "    # Recreate the directory\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "    \n",
    "    for file_id in data:\n",
    "        text = data[file_id]['text']\n",
    "        annotations = data[file_id]['annotations']\n",
    "        \n",
    "        # Tokenizing\n",
    "        tokens, token2text, sentence_breaks = split_text(text)\n",
    "\n",
    "        # Initialize the tags\n",
    "        tags = ['O'] * len(tokens)\n",
    "\n",
    "        ann_pos = 0\n",
    "        token_pos = 0\n",
    "\n",
    "        while ann_pos < len(annotations) and token_pos < len(tokens):\n",
    "\n",
    "            label = annotations[ann_pos]['label']\n",
    "            start = annotations[ann_pos]['start']\n",
    "            end = annotations[ann_pos]['end']\n",
    "\n",
    "            if filtered_entities:\n",
    "                if label not in filtered_entities:\n",
    "                    # increment to access next annotation\n",
    "                    ann_pos += 1\n",
    "                    continue\n",
    "            \n",
    "            ann_word = text[start:end]\n",
    "\n",
    "            # find the next word that fall between the annotation start and end\n",
    "            while token_pos < len(tokens) and token2text[token_pos][0] < start:\n",
    "                \n",
    "                token_pos += 1\n",
    "\n",
    "            if tokens[token_pos] == ann_word or \\\n",
    "                ann_word in tokens[token_pos] or \\\n",
    "                re.sub(r'\\W+', '', ann_word) in re.sub(r'\\W+', '', tokens[token_pos]):\n",
    "                tag_token(tokens, tags, token_pos, label)\n",
    "            elif ann_word in tokens[token_pos - 1] or \\\n",
    "                ann_word in tokens[token_pos - 1] or \\\n",
    "                re.sub(r'\\W+', '', ann_word) in re.sub(r'\\W+', '', tokens[token_pos - 1]):\n",
    "                tag_token(tokens, tags, token_pos - 1, label)\n",
    "            else:\n",
    "                print(tokens[token_pos], tokens[token_pos - 1], ann_word, label)\n",
    "\n",
    "            # increment to access next annotation\n",
    "            ann_pos += 1\n",
    "\n",
    "        # write to bio file\n",
    "        write_bio_files(os.path.join(output_dir, f\"{file_id}.bio\"), tokens, tags, sentence_breaks)\n",
    "    print(\"Conversion complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "559018a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion complete\n"
     ]
    }
   ],
   "source": [
    "convert_ann_to_bio(data, data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff73267",
   "metadata": {},
   "source": [
    "**Data cleaning:**\n",
    "\n",
    "* `remove_trailing_punctuation` function:\n",
    "    * removes trailing punctuation from a token\n",
    "    \n",
    "* `split_text` function:\n",
    "    * tokenizes text using regular expressions\n",
    "    * removes trailing punctuation from each token using remove_trailing_punctuation\n",
    "\n",
    "**Conversion to BIO:**\n",
    "\n",
    "* `tag_token` function:\n",
    "    * modifies a list of tags by adding a tag label to a token at a given position in the list\n",
    "    * the tag label is based on the position of the previous token and whether the current token has the same tag label as the previous token\n",
    "\n",
    "* `write_bio_files` function:\n",
    "    * writes the tags to a .bio file\n",
    "* `convert_ann_to_bio` function:\n",
    "    * converts annotations from a dictionary of text files to a BIO-tagged sequence\n",
    "    * filters entities based on a provided list of entity labels\n",
    "    * tokenizes the text using the split_text function\n",
    "    * initializes the tags to 'O'\n",
    "    * iterates through the annotations and tags the corresponding tokens using the tag_token function\n",
    "    * writes the BIO-tagged tokens and tags to a .bio file in the provided output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa04025",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
